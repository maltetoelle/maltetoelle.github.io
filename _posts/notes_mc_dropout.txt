By following their derivation we will now first introduce a slight change in notation: $\mathbf{M}$ and $\mathbf{m}$ will be used for deterministic matrices (used as weights and biases respectively) and $\mathbf{W}$ and $\mathbf{b}$ as their random counterparts (used as random variables inducing a distribution over Bayesian neural network's weights). 

$$
\begin{aligned}
\mathbf{M} &= \left\{\mathbf{M}_1 \in \mathbb{R}^{K_0\times K_1}, \mathbf{M}_2 \in \mathbb{R}^{K_1\times K_2}, \mathbf{m} \in \mathbb{R}^{K_1}\right\} ~,\\
\boldsymbol{\theta} &= \left\{\mathbf{W}_1 \in \mathbb{R}^{K_0\times K_1}, \mathbf{W}_2 \in \mathbb{R}^{K_1\times K_2}, \mathbf{b} \in \mathbb{R}^{K_1}\right\} ~.
\end{aligned}
$$

Our network for sketching out the derivation is rather simple

$$\hat{\mathbf{y}} = \left(\sqrt{\frac{1}{K_1}}\varphi\left(\left(\mathbf{x} \odot \hat{\mathbf{z}}_1\right)\mathbf{M}_1 + \mathbf{m}\right)\right)\odot\hat{\mathbf{z}}_2\mathbf{M}_2 ~,$$

it consists of two layers only and the output is scaled by $\sqrt{1/K_1}$. Since in a Bayesian neural network the stochasticity in the output is a result of the uncertainty in the model parameters we must transform the applied dropout from the feature space to the parameter space

$$\hat{\mathbf{y}} = \left(\sqrt{\frac{1}{K_1}}\varphi\left(\mathbf{x}\, \textrm{diag}\left(\hat{\mathbf{z}}_1\right)\mathbf{M}_1 + \mathbf{m}\right)\right)\textrm{diag}\left(\hat{\mathbf{z}}_2\right)\mathbf{M}_2 ~,$$

with the $\textrm{diag}(\cdot)$ operator mapping a vector to a diagonal matrix whose diagonal consists of the elements of the vector. In the following we will denote $\mathbf{z}_i$ as $\textrm{diag}(\mathbf{z}_i)$. The goal is to show that training an NN with dropout applied and the usual likelihood loss function with weight decay

$$
\mathcal{L}(\boldsymbol{\theta}) = -\frac{1}{N} \sum_{n=1}^{N} (\mathbf{y}_n -\hat{\mathbf{y}}_n)^2 + \lambda \sum_{l=1}^{L}||\mathbf{M}_l||_2^2 + ||\mathbf{m}_l||_2^2
$$

is equivalent to approximate variational inference in a Gaussian process, which is the same as minimizing the (log) evidence lower bound. An introduction to Gaussian processes can be found in this <a href="">post</a>.

### Gaussian Process Approximation

The first step is to define an approximate coveriance kernel for the Gaussian process approsimation of our first layer

$$
\begin{aligned}
\kappa(\mathbf{x}_i,\mathbf{x}_j) &= \int p(\mathbf{w}_{1,k})p(\mathbf{b})(\varphi\mathbf{w}_{1,k}^T\mathbf{x}_i + \mathbf{b}) (\varphi\mathbf{w}_{1,k}^T\mathbf{x}_j + \mathbf{b}) \,\mathrm{d}\mathbf{w}_{1,k} \\
&\approx \frac{1}{K_1} \sum_{k=1}^{K_1} (\varphi\mathbf{w}_{1,k}^T\mathbf{x}_i + \mathbf{b}) (\varphi\mathbf{w}_{1,k}^T\mathbf{x}_j + \mathbf{b}) \\
&= \boldsymbol{\Phi}^T\boldsymbol{\Phi} \quad \textrm{with} \quad \Phi\left(\mathbf{x}_i, \mathbf{W}_1, \mathbf{b} \right) = \sqrt{\frac{1}{K_1}}\left( \mathbf{W}_1^T\mathbf{x}_i + \mathbf{b} \right) \quad \textrm{and} \quad \boldsymbol{\Phi}=\left[\Phi_1,...,\Phi_n\right]^T ~.
\end{aligned}
$$

Having defined our covariance kernel the Gaussian process model is thus given by

$$
\begin{aligned}
\mathbf{f} | \mathbf{X}, \mathbf{W}_1 &\sim \mathcal{N}(\mathbf{0},\boldsymbol{\Phi}^T\boldsymbol{\Phi}) ~, \\
\mathbf{Y} | \mathbf{F} &\sim \mathcal{N}(\mathbf{f},\tau^{-1}\mathbf{I}) ~,
\end{aligned}
$$

which is similar noisy Gaussian process regression. $\tau^{-1}$ denotes the homoscedastic aleatoric noise (is constant across the whole input domain). Thus, our predictive distribution is given by

$$
\begin{aligned}
p(\mathbf{Y}|\mathbf{X}) &= \int p(\mathbf{Y}|\mathbf{f})p(\mathbf{f}|\mathbf{W}_1,\mathbf{b},\mathbf{X})p(\mathbf{W}_1)p(\mathbf{b}) \\
&= \int \mathcal{N}(\mathbf{Y}|\mathbf{0},\boldsymbol{\Phi}\boldsymbol{\Phi}^T + \tau^{-1}\mathbf{I})p(\mathbf{W}_1)p(\mathbf{b})\,\mathrm{d}\mathbf{W}_1 \,\mathrm{d}\mathbf{b} ~,
\end{aligned}
$$

but we are still left with analytically integrating over $\mathbf{f}$. y introducing an auxiliary random variable $\mathbf{w}_{2,k}\sim\mathcal{N}(0,\mathbf{I}_{K_1}) \in \mathbb{R}^{K_1\times 1}$ each representing a row of $\mathbf{W}_2$ the normal distribution of $\mathbf{Y}$ becomes (Bishop, 2006)

$$
\mathcal{N}(\mathbf{Y}|\mathbf{0},\boldsymbol{\Phi}\boldsymbol{\Phi}^T + \tau^{-1}\mathbf{I}) = \int \mathcal{N}(\mathbf{y}|\mathbf{0},\boldsymbol{\Phi}\boldsymbol{\Phi}^T + \tau^{-1}\mathbf{I}) \mathcal{N}(\mathbf{w}_{2,k}|0,\mathbf{I}_{K_1})\,\mathrm{d}\mathbf{w}_{2,k} ~.
$$

and our final predictive distribution is
$$
p(\mathbf{Y}|\mathbf{X}) = \int p(\mathbf{Y}|\mathbf{X},\mathbf{W}_2,\mathbf{W}_1,\mathbf{b})p(\mathbf{W}_1)p(\mathbf{W}_2)p(\mathbf{b}) ~,
$$

where now the integration is w.r.t. $\mathbf{W}_1$, $\mathbf{W}_2$ and $\mathbf{b}$.

### Variational Distribution


