<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2024-11-20T09:15:28+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Malte Tölle / Personal Site</title><subtitle>PhD Student in Computer Science</subtitle><author><name>Malte Tölle</name><email>malte.toelle@med.uni-heidelberg.de</email></author><entry><title type="html">Bayesian Neural Networks with Variational Inference</title><link href="http://0.0.0.0:4000/posts/2023/13/bayesian-neural-networks-with-variational-inference/" rel="alternate" type="text/html" title="Bayesian Neural Networks with Variational Inference" /><published>2023-05-13T14:58:00+00:00</published><updated>2023-05-13T14:58:00+00:00</updated><id>http://0.0.0.0:4000/posts/2023/13/BNNs_with_VI</id><content type="html" xml:base="http://0.0.0.0:4000/posts/2023/13/bayesian-neural-networks-with-variational-inference/"><![CDATA[<p>The general concept of variational inference (VI) and one application to linear regression can be found <a href="">here</a>. We will quickly review the basic concepts needed to understand the following.</p>

<p>In contrast to maximum likelihood, which assumes one true parameter set \(\hat{\boldsymbol{\theta}}\) that best explains our data, the Bayesian approach imposes a prior distribution onto the parameters and thereby treating them as random variables. A comparison for the simplest case of linear regression can be found <a href="https://maltetoelle.github.io/linear/regression/2020/10/27/try.html">here</a>. For a data set \(\mathcal{D}=(\mathbf{x}_i,\mathbf{y}_i)\) Bayes theorem is given by</p>

\[p(\boldsymbol{\theta}|\mathcal{D})=\frac{p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathcal{D})}=\frac{p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})}{\int p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})\,d\boldsymbol{\theta}}~,\]

<p>where the posterior of our model parameters \(p(\boldsymbol{\theta}\vert\mathcal{D})\) is obtained by multiplying the likelihood \(p(\mathcal{D}\vert\boldsymbol{\theta})\), the probability for seeing this data with our model parameters, with the prior \(p(\boldsymbol{\theta})\), our assumption for the parameter distribution of the parameters before seeing any data.</p>

<p>This can also be an uniform (non-informative) prior, assigning the same probability to all parameter distributions. To obtain a valid probability distribution the product of the two must be normalized to integrate to one, which is done by dividing by the evidence \(p(\mathcal{D})\) that is obtained by marginalizing out all possible parameter distributions. Even for simple models that are non-linear this calculation becomes intractable. So we must help ourselves with approximation frameworks such as variational inference or Markov Chain Monte Carlo (MCMC) sampling, which was compared to linear regression with variational inference in the same <a href="">post</a>.</p>

<p>The predictive distribution for a new data point \((\mathbf{x}_*,\mathbf{y}_*)\) is obtained in the Bayesian framework by marginalizing out the posterior parameter distribution</p>

\[p(\mathbf{y}_*|\mathbf{x}_*,\mathcal{D})=\int p(\mathbf{y}_*|\mathbf{x}_*,\mathcal{D},\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathcal{D})\,d\boldsymbol{\theta}~.\]

<p>In VI we use a simpler distribution \(q(\boldsymbol{\theta})\) to approximate our intractable posterior. The optimization objective is then given by the minimum of the KL divergence between approsimate and true posterior</p>

\[F(q):=\mathrm{KL}(q(\boldsymbol{\theta})||p(\boldsymbol{\theta}|\mathcal{D}))=\int q(\boldsymbol{\theta})\log \frac{q(\boldsymbol{\theta})}{p(\boldsymbol{\theta}|\mathcal{D})}\,\mathrm{d}\boldsymbol{\theta} \longrightarrow \underset{q(\boldsymbol{\theta}) \in \mathcal{Q}}{\min} ~.\]

<p>Although it is not a true distance measure because of its asymmetry, it can be seen as one, as it has its minimum of zero, if and only if both distributions are equal. For all other distributions it is always greater than zero. Befor we dive deeper into the derivations for applying VI to neural networks, we will quickly revisit the important properties of the KL divergence, as they come in handy later.</p>

<h2 id="kl-divergence">KL Divergence</h2>

<p>The KL divergence is defined as (<a href="https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729694">Kullback and Leibler 1951</a>)</p>

\[\begin{align}
\mathrm{KL}(q(x)||p(x)) &amp;= \textrm{H}(q(x),p(x)) - \textrm{H}(q(x)) \\
&amp;= - \int q(x) \log p(x)\,dx - \left( - \int q(x)\log q(x)\,dx \right) \\
&amp;= \int q(x)\log\frac{q(x)}{p(x)}\,dx~,
\end{align}\]

<p>where \(\textrm{H}(q(x),p(x))\) denotes the cross-entropy between \(q\) and \(p\) and \(\textrm{H}(q(x))\) is the entropy of \(q\). Formally, the KL divergence measures, how much information is lost, when p is approximated by q or vice versa. For two Gaussian distributions the KL divergence can be computed analytically</p>

\[\begin{aligned}
\mathrm{KL}(q(x)||p(x)) &amp;= \int q(x)\log q(x)\,dx - \int q(x)\log p(x)\,dx\\
&amp;= \frac{1}{2}\log\left(2\pi\sigma_p^2\right) + \frac{\sigma_q^2 + \left( \mu_q - \mu_p \right)^2}{2\sigma_p^2} - \frac{1}{2}\left( 1 + \log\left(2\pi\sigma_q^2\right) \right)\\
&amp;= \log\frac{\sigma_p}{\sigma_q} + \frac{\sigma_q^2 + \left( \mu_q - \mu_p \right)^2}{2\sigma_p^2} - \frac{1}{2} ~.
\end{aligned}\]

<p>For more complicated distributions, where the KL divergence is not analytically tractable, but the expectation can be approximated using Monte Carlo (MC) samples:</p>

\[\mathrm{KL}(q(x)||p(x)) = \mathbb{E}_{x\sim q}\left[ \log\frac{q(x)}{p(x)} \right] \approx \sum_{i=0}^{N}\left( \log q(x_i) - \log p(x_i) \right)~.\]

<p>An example for a more complicated distribution is the mixture of Gaussians:</p>

\[p(x)=\sum_i \pi_i \mathcal{N}(x|\mu_i,\sigma_i)~.\]

<p>The important properties of the KL divergence are:</p>

\[\begin{aligned}
    \textrm{non-negativity}&amp;: \quad \mathrm{KL}(q(x)||p(x)) \geq 0 ~, \;\; \forall x ~,\\
    \textrm{equality}&amp;: \quad \mathrm{KL}(q(x)||p(x)) = 0 \quad \textrm{if and only if} \quad q(x) = p(x) ~,\\
   \textrm{asymmetry}&amp;: \quad \mathrm{KL}(q(x)||p(x)) \neq \mathrm{KL}(p(x)||q(x)) ~.
\end{aligned}\]

<p>The KL divergence becomes zero, if and only if both distributions are equal. For all other distributions it is always greater than zero. As already said it is not a true distance metric becaus of its asymmetry. We distinguish between the reverse (exclusive) \(\mathrm{KL}(q(x)\vert\vert p(x))\) and the forward (inclusive) \(\mathrm{KL}(p(x)\vert\vert q(x))\) case. The following illustrates both cases.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.distributions.distribution</span> <span class="kn">import</span> <span class="n">Distribution</span>
<span class="kn">from</span> <span class="nn">torch.distributions.normal</span> <span class="kn">import</span> <span class="n">Normal</span>

<span class="k">def</span> <span class="nf">mc_kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">kl</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">rsample</span><span class="p">()</span>
        <span class="n">kl</span> <span class="o">+=</span> <span class="n">p</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="n">q</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">kl</span> <span class="o">/</span> <span class="n">n_samples</span>

<span class="k">class</span> <span class="nc">MixtureNormal</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">pi</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MixtureNormal</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">loc</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi</span><span class="p">))</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi</span><span class="p">))</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">loc</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">dists</span> <span class="o">=</span> <span class="p">[</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span> <span class="k">for</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">loc</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">rsample</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">pi</span><span class="p">,</span> <span class="n">dist</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dists</span><span class="p">):</span>
            <span class="n">rsample</span> <span class="o">+=</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">dist</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">dist</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">rsample</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">pdf</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">pi</span><span class="p">,</span> <span class="n">dist</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dists</span><span class="p">):</span>
            <span class="n">pdf</span> <span class="o">+=</span> <span class="n">pi</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">dist</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="p">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s">"ignore"</span><span class="p">,</span> <span class="nb">UserWarning</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="p">.</span><span class="nb">set</span><span class="p">()</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">matplotlib</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'text.usetex'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">matplotlib</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'text.latex.preamble'</span><span class="p">]</span> <span class="o">=</span> <span class="sa">r</span><span class="s">'\usepackage{bm}'</span>

<span class="k">def</span> <span class="nf">optimize_kl</span><span class="p">(</span><span class="n">p</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span> <span class="n">intial_mu_q</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">intial_sigma_q</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'forward'</span><span class="p">,</span>
                <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>

    <span class="c1"># intial values for mu_q and sigma_q
</span>    <span class="n">mu_q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">intial_mu_q</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">sigma_q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">intial_sigma_q</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">mu_q</span><span class="p">,</span> <span class="n">sigma_q</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">)):</span>
        <span class="n">optim</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu_q</span><span class="p">,</span> <span class="n">sigma_q</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">kl_type</span> <span class="o">==</span> <span class="s">'forward'</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">mc_kl_divergence</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">mc_kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mu_q</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">sigma_q</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">1.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">MixtureNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="p">[</span><span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">],</span> <span class="n">pi</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">kl_type</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s">'reverse'</span><span class="p">,</span> <span class="s">'forward'</span><span class="p">]):</span>
    <span class="n">mu_q</span><span class="p">,</span> <span class="n">sigma_q</span> <span class="o">=</span> <span class="n">optimize_kl</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">intial_mu_q</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">intial_sigma_q</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu_q</span><span class="p">,</span> <span class="n">sigma_q</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">exp</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">'$p(x)$'</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">q</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">exp</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">'$q(x)$'</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$p(x)$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="n">kl_type</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 1000/1000 [00:37&lt;00:00, 26.65it/s]
100%|██████████| 1000/1000 [00:30&lt;00:00, 32.34it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_3_1.png" alt="png" /></p>

<p>To better understand the two behaviours we brake down the KL divergence piece by piece. As can be seen in the reverse case the KL divergence approximates the mode of the \(p\) and thereby leaving a lot of mass uncovered. As we would like to minimize the KL divergence we must minimize the \(\log\frac{q}{p}\) term, which is small in areas, where \(p\) is large. Thereby the objective converges to a mode of \(p\). Consequently, in the forward case on the other hand the objective gets small in areas, where \(p\) is small.</p>

<h2 id="log-evidence-lower-bound">(Log) Evidence Lower Bound</h2>

<p>One can rewrite the log evidence such that we are left with the KL divergence from above and another term, named the (log) evidence lower bound or short ELBO</p>

\[\begin{aligned}
\log p(\mathcal{D}) &amp;= \int q(\boldsymbol{\theta})\log\frac{p(\mathcal{D},\boldsymbol{\theta})}{q(\boldsymbol{\theta})}\,d\boldsymbol{\theta} + \int q(\boldsymbol{\theta})\frac{q(\boldsymbol{\theta})}{p(\boldsymbol{\theta}|\mathcal{D})}\,d\boldsymbol{\theta} \\
&amp;= \textrm{ELBO}(q(\boldsymbol{\theta})) + \mathrm{KL}(q(\boldsymbol{\theta})||p(\boldsymbol{\theta}|\mathcal{D})) \\
&amp;\geq \textrm{ELBO}(q(\boldsymbol{\theta})) ~.
\end{aligned}\]

<p>As the KL divergence is always greater than zero, maximizing the first term, the ELBO, w.r.t. \(q\) is essentially equal to minimizing the second term, the KL divergence.</p>

<p>When applying the ELBO as optimization criterion to neural networks it must further be simplified into a data-dependent likelihood term and a regularizer measuring the “distance” between prior and posterior.</p>

\[\begin{aligned}
\textrm{ELBO}(q(\boldsymbol{\theta})) &amp;= \int q(\boldsymbol{\theta}) \log \frac{p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})}{q(\boldsymbol{\theta})}\,d\boldsymbol{\theta}  \\
&amp;= \int q(\boldsymbol{\theta}) \log p(\mathcal{D}|\boldsymbol{\theta})\,d\boldsymbol{\theta} + \int q(\boldsymbol{\theta})\log\frac{p(\boldsymbol{\theta})}{q(\boldsymbol{\theta})}\,d\boldsymbol{\theta} \\
&amp;= \underbrace{\mathbb{E}_{\boldsymbol{\theta}\sim q} \log p(\mathcal{D}|\boldsymbol{\theta})}_{\textrm{likelihood term}} - \underbrace{\mathrm{KL}(q(\boldsymbol{\theta})||p(\boldsymbol{\theta}))}_{\textrm{regularizer}}
\end{aligned}\]

<p>In each optimization step we sample weights \(\boldsymbol{\theta}\) from our approximate posterior \(q(\boldsymbol{\theta})\). The expectation of the likelihood term then measures, how well on average our model fits the data. The KL divergence measures how well our approximate posterior matches our prior. Since it must be minimized in order to minimize the ELBO, the model tries to keep the posterior as close as possible to the prior. For computing the above KL divergence we can now either use the forward or reverse version.</p>

<p>Now, the question remains to which family of distributions \(\mathcal{Q}\) to restrict \(q\) to allow for tractable soluitons for approximating the true posterior. We can either use a parametric distribution \(q_{\boldsymbol{\omega}}(\boldsymbol{\theta})\) governed by a set of parameters \(\boldsymbol{\omega}\). Hence, the ELBO becomes a function of \(\boldsymbol{\omega}\), and we can exploit standard non-linear optimization techniques to determine the optimal values for the parameters. Another possibility is to use factorized distributions. We will revisit both concepts in the domain of neural networks and apply them to the case of non-linear regression.</p>

<h2 id="mean-field-assumption">Mean Field Assumption</h2>

<p>As was the case in the linear regression example we start by using a factorized distribution as approximation, known as mean field approximation</p>

\[q(\boldsymbol{\theta})=\prod_i q_i(\boldsymbol{\theta}_i)~,\]

<p>which discards covariances in the parameters because of the factorization, but leads to faster computation time as this decreases the number of parameters to optimize as well. To model each individual \(q_i(\boldsymbol{\theta}_i)\) we will use a Gaussian distribution \(\mathcal{N}(\boldsymbol{\theta}_i\vert\boldsymbol{\mu}_i,\boldsymbol{\sigma}_i)\) as this is justified under the Bayesian central limit theorem. As can be seen in the comparison of MCMC sampling and VI for linear regression this assumption usually leads to an underestimation of the variance in the parameters.</p>

<p>When we want to apply the mean field approximation to a neural network a problem arises, when we want to apply the backpropagation algorithm to a probability distribution. The general update formula of gradient descent, which lies at the heart of backpropagation, is given by</p>

\[\theta_{ij}^* = \theta_{ij} - \eta \frac{\partial\mathcal{L}(\boldsymbol{\theta})}{\partial\theta_{ij}}~,\]

<p>where the new value \(\theta_{ij}^*\) for each weight after every iteration is obtained by subtracting the partial derivate of some loss function \(\mathcal{L}(\boldsymbol{\theta})\) (e.g. mean squared error, cross-entropy) w.r.t. that particular weight \(\theta_{ij}\) weighted by a learning rate \(\eta\) from \(\theta_{ij}\). The above formula can be applied to point estimates of the parameters only, each weight must have one explicitly defined value. Following from that, it is not applicable to probability distributions in their standard form.</p>

<h3 id="local-reparametrization-trick">(Local) Reparametrization Trick</h3>

<p>Remedy comes in the form of the reparametrization trick, which separates the deterministic and stochastic components of the weights (<a href="https://arxiv.org/abs/1312.6114">Kingma and Welling 2013</a>, <a href="https://arxiv.org/abs/1505.05424">Blundell et al. 2015</a>). Instead of sampling weights directly from \(q(\boldsymbol{\theta})\) the mean and the variance of the Gaussians modelling the weights are treated as parameters and another random variable \(\boldsymbol{\epsilon}\) is introduced:</p>

\[\begin{gathered}
\theta_{ij} \sim \mathcal{N}(\mu_{ij},\sigma_{ij}) ~,\\
\theta_{ij} = \mu_{ij} + \sigma_{ij}\epsilon_{ij} \quad \textrm{with} \quad \epsilon_{ij} \sim \mathcal{N}(0,1) ~.
\end{gathered}\]

<p>In each forward pass the weights are sampled according to the formula above and then, subsequently, the partial derivate w.r.t. to mean and variance is computed. This essentially means the number of parameters is doubled. To ensure an always positive variance, the reparametrization is usually extended with the Softplus function:</p>

\[\theta_{ij}=\mu_{ij} + \log\left( 1+ \exp\left(\sigma_{ij}^2\right) \right)\epsilon_{ij}~.\]

<p>The following images explain the induced differences of using reparametrization visually. On the left the initial situation is presented, where each weight \(\theta\) is modelled with a distribution \(q\). On the right the reparametrization trick is shown, in which in each forward pass a weight is sampled.</p>

<div>
  <img src="/assets/imgs/BNNs_with_VI_files/usual-1.png" width="200" />
  <img src="/assets/imgs/BNNs_with_VI_files/rt-1.png" width="200" />
</div>

<p>The reparametrization trick described above still exhibits limitations concerning the variance. If we were to sample one weight for each mini-batch, the resulting outputs would show high covariances. We could circumvent this problem by sampling a separate weight for each sample in the mini-batch, but this is computational expensive. <a href="https://arxiv.org/abs/1506.02557">Kingma et al. (2015)</a> first discovered that for a factorized Gaussian posterior on the weights, the posterior on the activations is also a factorized Gaussian. Thus, instead of sampling the weights directly we can also sample from the pre-activation neuron. <a href="https://arxiv.org/abs/1506.02557">Kingma et al. (2015)</a> report much lower variance and computational time for their gradient estimator termed local reparametrization trick. We will conduct a comparison later, when we have implemented both. More formally, their reparametrization trick is mathematically given by</p>

\[\begin{gathered}
q_{\omega}(\theta_{ij})=\mathcal{N}(\mu_{ij},\sigma_{ij}^{2}) \;\; \forall \;\; \theta_{ij} \in \boldsymbol{\theta} \quad \Longrightarrow \quad q_{\omega}(b_{mj}|\mathbf{A})=\mathcal{N}(\gamma_{mj},\delta_{mj}) ~,\\
\gamma_{mj}=\sum_{i} a_{mi}\mu_{ij} ~,\\
\delta_{mj}=\sum_{i} a_{mi}^{2}\sigma_{ij}^{2} ~,\\
b_{mj}=\gamma_{mj}+\sqrt{\delta_{mj}}\epsilon_{mj}\quad\textrm{with}\quad \epsilon_{mj} \sim \mathcal{N}(0,1) ~,
\end{gathered}\]

<p>where \(\boldsymbol{\epsilon}\) is a matrix of the same size as \(\mathbf{B}\). The difference between the two reparametrization tricks is visualized in the following.</p>

<div>
  <img src="/assets/imgs/BNNs_with_VI_files/rt-1.png" width="200" />
  <img src="/assets/imgs/BNNs_with_VI_files/lrt-1.png" width="200" />
</div>

<h3 id="implementation">Implementation</h3>

<h4 id="general-vi-module">General VI Module</h4>

<p>To later be able to extend the concept to the local reparametrization trick, we start by implementing a general VI module, which lies at the heart of all further layers.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torch.distributions.kl</span> <span class="kn">import</span> <span class="n">kl_divergence</span>

<span class="k">class</span> <span class="nc">VIModule</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">layer_fct</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                 <span class="n">weight_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
                 <span class="n">bias_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'reverse'</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">VIModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># function for forward pass e.g. F.linear, F.conv2d
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layer_fct</span> <span class="o">=</span> <span class="n">layer_fct</span>

        <span class="c1"># fall back to default vals
</span>        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">prior</span> <span class="o">=</span> <span class="p">{</span><span class="s">'mu'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">}</span>

        <span class="k">if</span> <span class="n">posteriors</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">posteriors</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">'mu'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
                <span class="s">'rho'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
            <span class="p">}</span>

        <span class="c1"># if prior is ScaleMixture we must use MC integration for KL div.
</span>        <span class="c1"># otherwise we can compute KL div. analitically
</span>        <span class="k">if</span> <span class="s">'pi'</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">prior</span><span class="p">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_kl_divergence</span> <span class="o">=</span> <span class="n">mc_kl_divergence</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">MixtureNormal</span><span class="p">(</span><span class="n">prior</span><span class="p">[</span><span class="s">'mu'</span><span class="p">],</span> <span class="n">prior</span><span class="p">[</span><span class="s">'sigma'</span><span class="p">],</span> <span class="n">prior</span><span class="p">[</span><span class="s">'pi'</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_kl_divergence</span> <span class="o">=</span> <span class="n">kl_divergence</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">prior</span><span class="p">[</span><span class="s">'mu'</span><span class="p">],</span> <span class="n">prior</span><span class="p">[</span><span class="s">'sigma'</span><span class="p">])</span>

        <span class="c1"># either 'forward' or 'reverse'
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">kl_type</span> <span class="o">=</span> <span class="n">kl_type</span>

        <span class="c1"># save parameters for resetting
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">posterior_mu_initial</span> <span class="o">=</span> <span class="n">posteriors</span><span class="p">[</span><span class="s">'mu'</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">posterior_rho_initial</span> <span class="o">=</span> <span class="n">posteriors</span><span class="p">[</span><span class="s">'rho'</span><span class="p">]</span>

        <span class="c1"># initialize weights and biases
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">W_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">weight_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_rho</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">weight_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bias_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">bias_size</span><span class="p">))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias_rho</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">bias_size</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'bias_mu'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'bias_rho'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

        <span class="c1"># reset
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_mu</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_mu_initial</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_rho</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_rho_initial</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_mu_initial</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias_rho</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_rho_initial</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">kl</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># compute KL div. by instantiating the weights as Normal distribution
</span>        <span class="n">_kl</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W_mu</span><span class="p">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W_rho</span><span class="p">).</span><span class="n">cpu</span><span class="p">())).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">_kl</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span><span class="p">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_rho</span><span class="p">).</span><span class="n">cpu</span><span class="p">())).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">_kl</span>

    <span class="k">def</span> <span class="nf">kl_divergence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
                      <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'reverse'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># either reverse or forward KL div.
</span>        <span class="k">if</span> <span class="n">kl_type</span> <span class="o">==</span> <span class="s">'reverse'</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_kl_divergence</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">rsample</span><span class="p">(</span><span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># reparametrization trick
</span>        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">mu</span><span class="p">.</span><span class="n">size</span><span class="p">()).</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">mu</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">sigma</span>
</code></pre></div></div>

<p>In this post we will be using linear layers only, but the module can easily be extended to a convolutional layer by changing the layer function. The VI module can be given a prior as well as a posterior distribution. Dependend on whether a mixture of Gaussians shall be used as prior, the KL divergence is either computed analytically with the version provided by PyTorch or our MC KL divergence is used. Further we can specify, whether the forward or reverse KL divergence shall be computed. At last, it implements a function for performing reparametrization.</p>

<h4 id="local-reparametrization-trick-layer">(Local) Reparametrization Trick Layer</h4>

<p>Now we extend our VI module with the reparametrization trick and its local counterpart.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RTLayer</span><span class="p">(</span><span class="n">VIModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">layer_fct</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                 <span class="n">weight_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
                 <span class="n">bias_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'forward'</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">RTLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">layer_fct</span><span class="p">,</span>
                                      <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                      <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                      <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                      <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                      <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">)</span>
        <span class="c1"># these will be used for an easy extension to a convolutional layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># sample each weight with reparametrization trick
</span>        <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W_mu</span><span class="p">,</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W_rho</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span><span class="p">,</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_rho</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="c1"># use this weight for forward pass
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_fct</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LRTLayer</span><span class="p">(</span><span class="n">VIModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">layer_fct</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                 <span class="n">weight_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
                 <span class="n">bias_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'forward'</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">LRTLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">layer_fct</span><span class="p">,</span>
                                       <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                       <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                       <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                       <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                       <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">)</span>
        <span class="c1"># these will be used for an easy extension to a convolutional layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># first conduct forward pass for mean and variance
</span>        <span class="n">act_mu</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_fct</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_mu</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_sigma</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W_rho</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">bias_var</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_rho</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias_var</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">act_std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1e-16</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_fct</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias_var</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span><span class="p">))</span>
        <span class="c1"># sample from activation
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">act_mu</span><span class="p">,</span> <span class="n">act_std</span><span class="p">)</span>
</code></pre></div></div>

<p>The implementation for both the “normal” reparametrization layer (<code class="language-plaintext highlighter-rouge">RTLayer</code>) and the local reparametrization layer (<code class="language-plaintext highlighter-rouge">LRTLayer</code>) both look very similar. The only difference can be spotted in the forward function. As described earlier the RTLayer first samples the weight vector and then performs the forward pass. In contrast to that the LRTLayer first performs the forward pass for mean and variance and afterwards uses the reparametrization trick to sample from these activations.</p>

<h4 id="linear-layer">Linear Layer</h4>

<p>The extensions to real layers, linear and convolutional, is trivial now. The example for a linear layer can be found underneath, the implementation for the convolutional layers can be found at the end of the post.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LinearRT</span><span class="p">(</span><span class="n">RTLayer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'forward'</span><span class="p">):</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_featurs</span> <span class="o">=</span> <span class="n">out_features</span>

        <span class="n">weight_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">)</span>

        <span class="n">bias_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_features</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">LinearRT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="n">linear</span><span class="p">,</span>
                                        <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                        <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                        <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                        <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LinearLRT</span><span class="p">(</span><span class="n">LRTLayer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'forward'</span><span class="p">):</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_featurs</span> <span class="o">=</span> <span class="n">out_features</span>

        <span class="n">weight_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">)</span>

        <span class="n">bias_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_features</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">LinearLRT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="n">linear</span><span class="p">,</span>
                                        <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                        <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                        <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                        <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="loss-function">Loss Function</h4>

<p>Before we can start using these layers in a neural network, we must implement our loss function, the ELBO. As a loss function we use the full negative log likelihood. This has the advantage of being able to estimate the full uncertainty. We will not dive deeper into the derivations, this is done in another <a href="">post</a>. As a quick overview, uncertainty can be divided into uncertainty or noise inherent in the data, called aleatoric uncertainty, and uncertainty inherent in our model, termed epistemic uncertainty. The aleatoric uncertainty is captured implictly during training with our loss function and can also be estimated without employing Bayesian techniques. To capture epistemic uncertainty on the other hand we must impose distributions onto our parameters and follow the Bayesian approach.</p>

<p>To capture the aleatoric uncertainty our model \(\mathbf{f}_{\boldsymbol{\theta}}\) gains a new head</p>

\[\mathbf{f}_{\boldsymbol{\theta}}(\mathbf{x})=\left[ \hat{\mathbf{y}},\hat{\boldsymbol{\sigma}}^2 \right]\]

<p>and our loss function is then given by</p>

\[\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{D}\sum_i \frac{1}{2}\hat{\sigma}_i^2 \left( y_i - \hat{y}_i \right)^2 + \frac{1}{2}\log\hat{\sigma}_i^2 ~.\]

<p>Using this loss function we can implicitly learn the aleatoric uncertainty in our data (<a href="https://arxiv.org/abs/1703.04977">Kendall and Gal 2017</a>). For numerical stability in practice we let our model ouput the (negative) log variance as this allows for negative values as well:</p>

\[\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{D}\sum_i\frac{1}{2}\exp \left( -\log \hat{\sigma}_i^2 \right) \left( y_i - \hat{y}_i \right)^2 - \frac{1}{2}\log\hat{\sigma}_i^2 ~.\]

<p>In Regression \(D\) is set to the number of samples in the mini-batch, while in classification \(D=1\). To also account for epistemic uncertainty we need to compute the variance of the output using Monte Carlo integration</p>

\[\mathrm{Var}\left[\mathbf{y}\right]=\underbrace{\frac{1}{T}\sum_{t=1}^{T}\hat{\mathbf{y}}_t^2-\left(\frac{1}{T}\sum_{t=1}^{T}\hat{\mathbf{y}}_t \right)^2}_{\textrm{epistemic}}+\underbrace{\boldsymbol{\sigma}}_{\textrm{aleatoric}} ~,\]

<p>where \(\boldsymbol{\sigma}\) denotes the aleatoric part estimated with the full negative log likelihood.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gaussian_nll</span><span class="p">(</span><span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">logvar</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'mean'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logvar</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">mu</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">logvar</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">if</span> <span class="n">reduction</span><span class="o">==</span><span class="s">'mean'</span> <span class="k">else</span> <span class="n">loss</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">ELBO</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">train_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'regression'</span><span class="p">):</span>        
        <span class="nb">super</span><span class="p">(</span><span class="n">ELBO</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">train_size</span> <span class="o">=</span> <span class="n">train_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="s">'mean'</span> <span class="k">if</span> <span class="n">train_type</span> <span class="o">==</span> <span class="s">'regression'</span> <span class="k">else</span> <span class="s">'sum'</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">kl</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">gaussian_nll</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">target</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">train_size</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>

<span class="k">def</span> <span class="nf">calc_uncert</span><span class="p">(</span><span class="n">preds</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'mean'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">epi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">var</span><span class="p">(</span><span class="n">preds</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">preds</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">exp</span><span class="p">()</span>
    <span class="n">uncert</span> <span class="o">=</span> <span class="n">ale</span> <span class="o">+</span> <span class="n">epi</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s">'mean'</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ale</span><span class="p">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">epi</span><span class="p">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">uncert</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ale</span><span class="p">,</span> <span class="n">epi</span><span class="p">,</span> <span class="n">uncert</span>
</code></pre></div></div>

<h4 id="kl-divergence-reweighting">KL Divergence Reweighting</h4>

<p>The attentive reader may have noticed that we have reweighted both our likelihood as well as our KL divergence in the above code for the ELBO. A problem in training Bayesian neural networks with VI arises when we have a discrepancy between number of model parameters and data set size. Most often our number of parameters exceeds the number of training points leading to overfitting when performing maximum likelihood estimation. When utilizing the ELBO this means the magnitude of the KL divergence term, the regularizer, exceeds the likelihood cost and the training focusses on reducing the complexity instead of the likelihood. Thus, the KL divergence term must be reweighted by a factor \(\beta\):</p>

\[\textrm{ELBO}(q(\boldsymbol{\theta})) = \mathbb{E}_{\boldsymbol{\theta}\sim q} \log p(\mathcal{D}|\boldsymbol{\theta}) - \beta \mathrm{KL}(q(\boldsymbol{\theta})||p(\boldsymbol{\theta})) ~.\]

<p>A good value for \(\beta\) leads to an initial magnitude of the complexity cost comparable to the magnitude of the likelihood term. But considering only the likelihood disregards the number of model parameters, which highly influences the magnitude of the KL divergence. Following from this, the KL divergence must be scaled by the number of parameters to ensure its magnitude is only influenced by the approximation between posterior and prior. Reducing the number of parameters would otherwise decrease the complexity cost without increasing the model fit. At best, the likelihood is scaled by the number of data points in the data set to balance out both terms.</p>

<p>Since the use for the ELBO is motivated by enabling mini-batched training, the literature provides different \(\beta\) for training with mini-batches. The standard scaling factor was introduced by <a href="https://www.cs.toronto.edu/~graves/nips_2011.pdf">Graves (2011)</a>, which sets \(\beta=\frac{1}{M}\), where \(M\) denotes the number of mini-batches. A more sophisticated version is provided by <a href="https://arxiv.org/abs/1505.05424">Blundell et al. (2015)</a>:</p>

\[\beta_i = \frac{2^{M-i}}{2^M-1} ~, \quad i \in \{1,...,M\} ~,\]

<p>where \(i\) denotes the number of the current batch number. This condition ensures that \(\beta\) is not uniform across mini-batches but still sophisticates \(\sum_{i=1}^M \beta_i=1\). While assuming higher values for \(\beta\) at the beginning of training, the importance of the complexity costrapidly declines. By utilizing this approach the prior gets more influential in the beginning,before the training focuses on the likelihood, when more data is observed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="k">def</span> <span class="nf">get_beta</span><span class="p">(</span><span class="n">beta_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
             <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">beta_type</span> <span class="o">==</span> <span class="s">"Blundell"</span><span class="p">:</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">beta_type</span> <span class="o">==</span> <span class="s">"Standard"</span><span class="p">:</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">m</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">beta_type</span>
    <span class="k">return</span> <span class="n">beta</span>
</code></pre></div></div>

<p>Before we conduct experiments with the above described VI framework, we will revisit another very popular technique for approximate variational inference in neural networks that gains its popularity from its simplicity and fewer computational requirements.</p>

<h2 id="monte-carlo-dropout">Monte Carlo Dropout</h2>

<p>Coming from the rather intuitive way of modelling each weight with a Gaussian distribution to incorporate uncertainty into the model parameters, Monte Carlo (MC) dropout is less instinctive but bears some strong advantages. <a href="https://arxiv.org/abs/1506.02142">Gal and Ghahramani (2016)</a> proposed dropout as approximate Bayesian inference. Their idea is basically simple, instead of just applying dropout during training to prevent overfitting dropout is also applied during testing. This makes the output of the model a random variable to and we are able to quantify uncertainty is described earlier.</p>

<p>The derivation relies on variational inference and is based on some heavy assumptions about the prior. For the full proof the interested reader is referred to <a href="http://proceedings.mlr.press/v48/gal16-supp.pdf">Gal and Ghahramani (2015)</a>.</p>

<p>An MC dropout layer can easily be implemented in PyTorch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MCDropout</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MCDropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="experiments">Experiments</h3>

<p>As we have now revisited the most popular approxmation techniques with variational inference, we will now take them to use for the problem of non-linear regression. So, we first must generate our training data:</p>

\[y = 10 \sin(2\pi x) + \epsilon \quad \textrm{with} \quad \epsilon \sim \mathcal{N}(0,2) ~.\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma</span>
    <span class="k">return</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">epsilon</span>

<span class="n">train_size</span> <span class="o">=</span> <span class="mi">92</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">train_size</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training data'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'True'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_22_0.png" alt="png" /></p>

<p>Next we will generate helper functions that simplify training lateron. Also, we defined our function that plots the uncertainty and the mean prediction of our models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.ndimage</span> <span class="kn">import</span> <span class="n">gaussian_filter1d</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">Adam</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">loss_fct</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">beta_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">optim</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">,</span>
          <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="nb">list</span><span class="p">):</span>

    <span class="n">losses</span><span class="p">,</span> <span class="n">mses</span><span class="p">,</span> <span class="n">kls</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="n">optim</span> <span class="o">=</span> <span class="n">optim</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">total_loss</span><span class="p">,</span> <span class="n">total_kl</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">optim</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="n">out</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">[(</span><span class="n">batch_idx</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]])</span>

            <span class="n">beta</span> <span class="o">=</span> <span class="n">get_beta</span><span class="p">(</span><span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">[(</span><span class="n">batch_idx</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]],</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optim</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">total_kl</span> <span class="o">+=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

            <span class="n">mse</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">[(</span><span class="n">batch_idx</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]]).</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">mses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
        <span class="n">kls</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_kl</span><span class="p">)</span>
        <span class="n">grads</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">gradients</span><span class="p">())</span>

        <span class="n">pbar</span><span class="p">.</span><span class="n">set_description</span><span class="p">(</span><span class="s">'loss: %.6f'</span> <span class="o">%</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">mses</span><span class="p">,</span> <span class="n">kls</span><span class="p">,</span> <span class="n">grads</span>


<span class="k">def</span> <span class="nf">pred</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mc_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>

    <span class="n">y_preds</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">mc_samples</span><span class="p">)):</span>
            <span class="n">y_pred</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">y_preds</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_pred</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">y_preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[:,:,</span><span class="mi">0</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_preds</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ale</span><span class="p">,</span> <span class="n">epi</span><span class="p">,</span> <span class="n">uncert</span> <span class="o">=</span> <span class="n">calc_uncert</span><span class="p">(</span><span class="n">y_preds</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ale</span><span class="p">,</span> <span class="n">epi</span><span class="p">,</span> <span class="n">uncert</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">]),</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">]),</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">ale</span><span class="p">,</span> <span class="n">epi</span><span class="p">,</span> <span class="n">uncert</span>


<span class="k">def</span> <span class="nf">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">x_train</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_train</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">ale</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">epi</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">uncert</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>

    <span class="n">ale</span><span class="p">,</span> <span class="n">epi</span><span class="p">,</span> <span class="n">uncert</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ale</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">epi</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">uncert</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Predictive mean'</span><span class="p">);</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training data'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">flatten</span><span class="p">(),</span>
                    <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">y_pred_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">ale</span> <span class="o">+</span> <span class="n">epi</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
                    <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">y_pred_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">ale</span> <span class="o">+</span> <span class="n">epi</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
                    <span class="n">color</span><span class="o">=</span><span class="s">'#6C85B6'</span><span class="p">,</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Aleatoric uncertainty'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">flatten</span><span class="p">(),</span>
                    <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">y_pred_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">epi</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
                    <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">y_pred_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">epi</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
                    <span class="n">color</span><span class="o">=</span><span class="s">'#6C85B6'</span><span class="p">,</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Epistemic uncertainty'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$y$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>
</code></pre></div></div>

<h4 id="hyperparameters">Hyperparameters</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">LR</span> <span class="o">=</span> <span class="mf">0.08</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">200</span>
</code></pre></div></div>

<h3 id="frequentist-approach">Frequentist Approach</h3>

<p>Before we conduct experiments with variational inference, we will first take a look at the frequentist approach.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># as the training function expects the model to return the kl, we return 0 here
</span>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">()]).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_freq</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">loss_fct</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span> <span class="p">:</span> <span class="n">F</span><span class="p">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_freq</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                <span class="n">beta_type</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 3.760674: 100%|██████████| 200/200 [00:02&lt;00:00, 75.08it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">500</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">y_pred_mean_freq</span><span class="p">,</span> <span class="n">ale_freq</span><span class="p">,</span> <span class="n">epi_freq</span><span class="p">,</span> <span class="n">uncert_freq</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_freq</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_freq</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                  <span class="n">ale</span><span class="o">=</span><span class="n">ale_freq</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_freq</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_freq</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 4270.19it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_30_1.png" alt="png" /></p>

<p>As can be seen, the output of the frequentist model only has one defined value. Even in regions lacking training data the model confidently outputs one value as the only true one. Moving forward to a Bayesian approach, we will first conduct maximum a posterior inference utilizing the full negativ elog likelihood as loss function and by this equipping our model with the ability to quantify aleatoric uncertainty in the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_map</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">loss_fct</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span> <span class="p">:</span> <span class="n">gaussian_nll</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">out</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s">'mean'</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_map</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                   <span class="n">beta_type</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 2.771659: 100%|██████████| 200/200 [00:03&lt;00:00, 64.52it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_map</span><span class="p">,</span> <span class="n">ale_map</span><span class="p">,</span> <span class="n">epi_map</span><span class="p">,</span> <span class="n">uncert_map</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_map</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_map</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                  <span class="n">ale</span><span class="o">=</span><span class="n">ale_map</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_map</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_map</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 4455.30it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_33_1.png" alt="png" /></p>

<p>With this approach we are able to incorporate quantitative uncertainty estimates into the model’s prediction. The farther we get from the training data, the more the uncertainty in the model’s output increases. Still, this is only part of the uncertainty, we do still not consider the uncertainty inherent in the model. Thus, we will now move towards the fully Bayesian approach with variational inference.</p>

<h3 id="going-bayesian-with-monte-carlo-dropout">Going Bayesian with Monte Carlo Dropout</h3>

<p>The first experiments will be conducted with MC dropout to show its simplicity. This approach only needs a very simple adjustment, we only must use the MCDropout layer from above after every forward pass.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ModelDropout</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ModelDropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">mc_dropout</span> <span class="o">=</span> <span class="n">MCDropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mc_dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mc_dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mc_dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># as the training function expects the model to return the kl, we return 0 here
</span>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">()]).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>

<p>It is important to mention that we must always set a weight decay in the optimizer, since that determines the shape of our prior (together with the dropout rate).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">model_dropout</span> <span class="o">=</span> <span class="n">ModelDropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
<span class="n">loss_fct</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span> <span class="p">:</span> <span class="n">gaussian_nll</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">out</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s">'mean'</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_dropout</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                   <span class="n">beta_type</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 3.081201: 100%|██████████| 200/200 [00:03&lt;00:00, 56.73it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_dropout</span><span class="p">,</span> <span class="n">ale_dropout</span><span class="p">,</span> <span class="n">epi_dropout</span><span class="p">,</span> <span class="n">uncert_dropout</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_dropout</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_dropout</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                  <span class="n">ale</span><span class="o">=</span><span class="n">ale_dropout</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_dropout</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_dropout</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="c1">#plt.savefig('dropout_uncert.pdf', bbox_inches='tight')
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 1353.97it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_38_1.png" alt="png" /></p>

<p>Dropout makes it possible to quantify both, aleatoric and epistemic, uncertainty.</p>

<h3 id="bayes-by-backprop">Bayes by Backprop</h3>

<p>We will now use the VI layer we have defined earlier. Since we are modeling out prior and posterior explicitly (at the cost of doubling the number of weights), we have more freedom in designing the training procedure. The only difference to the models above is that we now have a KL divergence that must be computed for each forward pass</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ModelVI</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">reparam</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'lrt'</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ModelVI</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">LinearVI</span> <span class="o">=</span> <span class="n">LinearLRT</span> <span class="k">if</span> <span class="n">reparam</span> <span class="o">==</span> <span class="s">'lrt'</span> <span class="k">else</span> <span class="n">LinearRT</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">LinearVI</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">LinearVI</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">LinearVI</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">kl_div</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">kl_div</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">kl</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">children</span><span class="p">():</span>
            <span class="n">kl</span> <span class="o">+=</span> <span class="n">m</span><span class="p">.</span><span class="n">kl</span>
        <span class="k">return</span> <span class="n">kl</span>

    <span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">()]).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>

<p>In our first training procedure we will use a \(\beta=10^{-2}\) for both the “standard” and local reparametrization trick.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prior</span> <span class="o">=</span> <span class="p">{</span><span class="s">'mu'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">}</span>

<span class="n">posteriors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'mu'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
    <span class="s">'rho'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">elbo</span> <span class="o">=</span> <span class="n">ELBO</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">loss_fct</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="n">elbo</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

<span class="n">kls</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">mses</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>
</code></pre></div></div>

<h4 id="standard-reparametrization-trick">“Standard” Reparametrization Trick</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_vi_rt</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span> <span class="n">reparam</span><span class="o">=</span><span class="s">'rt'</span><span class="p">)</span>

<span class="n">no_params</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">model</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">p</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">()]))</span>
<span class="n">beta_type</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">no_params</span><span class="p">(</span><span class="n">model_vi_rt</span><span class="p">)</span>

<span class="n">losses_rt</span><span class="p">,</span> <span class="n">mses_rt</span><span class="p">,</span> <span class="n">kls_rt</span><span class="p">,</span> <span class="n">grads_rt</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_rt</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                             <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 341.115906: 100%|██████████| 200/200 [00:08&lt;00:00, 22.25it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_rt</span><span class="p">,</span> <span class="n">ale_rt</span><span class="p">,</span> <span class="n">epi_rt</span><span class="p">,</span> <span class="n">uncert_rt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_rt</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_rt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                  <span class="n">ale</span><span class="o">=</span><span class="n">ale_rt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_rt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_rt</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 945.32it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_45_1.png" alt="png" /></p>

<h4 id="local-reparametrization-trick-1">Local Reparametrization Trick</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_vi_lrt</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>

<span class="n">beta_type</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">no_params</span><span class="p">(</span><span class="n">model_vi_lrt</span><span class="p">)</span>

<span class="n">losses_lrt</span><span class="p">,</span> <span class="n">mses_lrt</span><span class="p">,</span> <span class="n">kls_lrt</span><span class="p">,</span> <span class="n">grads_lrt</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                                 <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>

<span class="n">kls</span><span class="p">[</span><span class="sa">r</span><span class="s">'\#weights'</span><span class="p">]</span> <span class="o">=</span> <span class="n">kls_lrt</span>
<span class="n">mses</span><span class="p">[</span><span class="sa">r</span><span class="s">'\#weights'</span><span class="p">]</span> <span class="o">=</span> <span class="n">mses_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 286.557373: 100%|██████████| 200/200 [00:10&lt;00:00, 19.43it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert_lrt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                  <span class="n">ale</span><span class="o">=</span><span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_lrt</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="c1">#plt.savefig('ffg_uncert.pdf', bbox_inches='tight')
</span><span class="n">preds</span><span class="p">[</span><span class="sa">r</span><span class="s">'\#weights'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_mean_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 623.07it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_48_1.png" alt="png" /></p>

<h4 id="comparing-gradients">Comparing Gradients</h4>

<p>With more training samples the figure underneath would get more meaningful, but also in this example it can bessen that the average gradients after each forward pass are lower for the LRT meaning our model does not oscillate that much around a lcoal optimum.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grads_rt</span><span class="p">)),</span> <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">grads_rt</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">'RT'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grads_lrt</span><span class="p">)),</span> <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">grads_lrt</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">'LRT'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'iteration'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'grad'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_50_0.png" alt="png" /></p>

<h3 id="scale-mixture-prior">Scale Mixture Prior</h3>

<p><a href="https://arxiv.org/abs/1505.05424">Blundell et al. (2015)</a> have shown that using a scale mixture prior (mixture of two Gaussians) can be benefitial for training, since then the model has the ability to have weights with a very small variance but can also match for uncertainty estimation the high variance part of the prior.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prior</span> <span class="o">=</span> <span class="p">{</span><span class="s">'mu'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="s">'sigma'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span> <span class="s">'pi'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]}</span>

<span class="n">posteriors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'mu'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
    <span class="s">'rho'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">model_vi_smp</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>

<span class="n">beta_type</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">no_params</span><span class="p">(</span><span class="n">model_vi_smp</span><span class="p">)</span>

<span class="n">losses_smp</span><span class="p">,</span> <span class="n">mses_smp</span><span class="p">,</span> <span class="n">kls_smp</span><span class="p">,</span> <span class="n">grads_smp</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_smp</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                       <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 192.050583: 100%|██████████| 200/200 [00:17&lt;00:00, 11.24it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_smp</span><span class="p">,</span> <span class="n">ale_smp</span><span class="p">,</span> <span class="n">epi_smp</span><span class="p">,</span> <span class="n">uncert_smp</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_smp</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_smp</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">ale</span><span class="o">=</span><span class="n">ale_smp</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_smp</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_smp</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 230.19it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_53_1.png" alt="png" /></p>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_53_2.png" alt="png" /></p>

<h3 id="different-betas">Different Betas</h3>

<p>Since the selection of \(\beta\) highly influences our training procedure, we will conduct experiments using different \(\beta\).</p>

<h4 id="graves">Graves</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prior</span> <span class="o">=</span> <span class="p">{</span><span class="s">'mu'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">}</span>
<span class="n">beta_type</span> <span class="o">=</span> <span class="s">"Standard"</span>
<span class="n">model_vi_lrt_standard</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">mses_lrt_standard</span><span class="p">,</span> <span class="n">kls_lrt_standard</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_standard</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
<span class="n">kls</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">kls_lrt_standard</span>
<span class="n">mses</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">mses_lrt_standard</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 406.891388: 100%|██████████| 200/200 [00:10&lt;00:00, 18.78it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert_lrt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_standard</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">ale</span><span class="o">=</span><span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_lrt</span><span class="p">)</span>
<span class="n">preds</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_mean_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 555.56it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_56_1.png" alt="png" /></p>

<h4 id="blundell">Blundell</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">beta_type</span> <span class="o">=</span> <span class="s">"Blundell"</span>
<span class="n">model_vi_lrt_blundell</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">mses_lrt_blundell</span><span class="p">,</span> <span class="n">kls_lrt_blundell</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_blundell</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
<span class="n">kls</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">kls_lrt_blundell</span>
<span class="n">mses</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">mses_lrt_blundell</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 323.418182: 100%|██████████| 200/200 [00:10&lt;00:00, 18.80it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert_lrt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_blundell</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">ale</span><span class="o">=</span><span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_lrt</span><span class="p">)</span>
<span class="n">preds</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_mean_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 457.78it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_59_1.png" alt="png" /></p>

<h4 id="no-reweighting-beta1">No Reweighting (\(\beta=1\))</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">beta_type</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">model_vi_lrt_1</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">mses_lrt_1</span><span class="p">,</span> <span class="n">kls_lrt_1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_1</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                    <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
<span class="n">kls</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">kls_lrt_1</span>
<span class="n">mses</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">mses_lrt_1</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 527.517944: 100%|██████████| 200/200 [00:10&lt;00:00, 19.60it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert_lrt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_1</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">ale</span><span class="o">=</span><span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_lrt</span><span class="p">)</span>
<span class="n">preds</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_mean_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 653.01it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_62_1.png" alt="png" /></p>

<h4 id="no-kl-divergence-beta0">No KL divergence (\(\beta=0\))</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">beta_type</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">model_vi_lrt_0</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">mses_lrt_0</span><span class="p">,</span> <span class="n">kls_lrt_0</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_0</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                    <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
<span class="n">kls</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">kls_lrt_0</span>
<span class="n">mses</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">mses_lrt_0</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 183.360672: 100%|██████████| 200/200 [00:10&lt;00:00, 19.51it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert_lrt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_0</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">ale</span><span class="o">=</span><span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_lrt</span><span class="p">)</span>
<span class="n">preds</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_mean_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 624.75it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_65_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s">'\#weights'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'Standard'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'Blundell'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
<span class="k">for</span> <span class="n">beta</span><span class="p">,</span> <span class="n">kl</span> <span class="ow">in</span> <span class="n">kls</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kl</span><span class="p">)),</span> <span class="n">kl</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="n">colors</span><span class="p">[</span><span class="n">beta</span><span class="p">]])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1695</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'iteration'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'KL$q((\bm{\theta})||p(\bm{\theta}))$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'diff_betas_kl.pdf'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_66_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">beta</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">preds</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="n">colors</span><span class="p">[</span><span class="n">beta</span><span class="p">]])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'training data'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'x'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'y'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'diff_betas_pred.pdf'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_67_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">beta</span><span class="p">,</span> <span class="n">mse</span> <span class="ow">in</span> <span class="n">mses</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mse</span><span class="p">)),</span> <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">mse</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="n">colors</span><span class="p">[</span><span class="n">beta</span><span class="p">]])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'iteration'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'MSE$(y;\hat{y})$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'diff_betas_nll.pdf'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_68_0.png" alt="png" /></p>

<h2 id="appendix">Appendix</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">conv2d</span>
<span class="kn">from</span> <span class="nn">torch.nn.modules.utils</span> <span class="kn">import</span> <span class="n">_pair</span>

<span class="k">class</span> <span class="nc">Conv2dRT</span><span class="p">(</span><span class="n">RTLayer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                 <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'reverse'</span><span class="p">):</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="n">weight_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">bias_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_channels</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2dRT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">conv2d</span><span class="p">,</span>
                                        <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                        <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                        <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                        <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">,</span>
                                        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Conv2dLRT</span><span class="p">(</span><span class="n">LRTLayer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                 <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'reverse'</span><span class="p">):</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="n">weight_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">bias_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_channels</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2dLRT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">conv2d</span><span class="p">,</span>
                                        <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                        <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                        <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                        <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">,</span>
                                        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name>Malte Tölle</name><email>malte.toelle@med.uni-heidelberg.de</email></author><category term="linear" /><category term="regression" /><category term="vi" /><category term="mcmc" /><summary type="html"><![CDATA[The general concept of variational inference (VI) and one application to linear regression can be found here. We will quickly review the basic concepts needed to understand the following.]]></summary></entry><entry><title type="html">Linear Regression with Variational Inference and Markov Chain Monte Carlo Sampling</title><link href="http://0.0.0.0:4000/linear/regression/vi/mcmc/compare_mcmc_vi/" rel="alternate" type="text/html" title="Linear Regression with Variational Inference and Markov Chain Monte Carlo Sampling" /><published>2023-03-13T10:47:00+00:00</published><updated>2023-03-13T10:47:00+00:00</updated><id>http://0.0.0.0:4000/linear/regression/vi/mcmc/compare_mcmc_vi</id><content type="html" xml:base="http://0.0.0.0:4000/linear/regression/vi/mcmc/compare_mcmc_vi/"><![CDATA[<p><a href="https://maltetoelle.github.io/linear/regression/2020/10/27/try.html">Here</a> you can fing more in depth information about the topic of linear regression in general. You might want to check it out, if you are unfamiliar with it. Here we will only review the most basic intuitions. In this post we will revisit two methods for approximating the intractable posterior in Bayesian inference, namely variational inference (VI) and Markov chain Monte Carlo (MCMC) sampling. While the latter is able to approximate the posterior exactly, it takes longer to converge. This can be eliminated with variational inference at the cost of no exact approximation. But more to come.</p>

<p>Given a dataset \(\mathcal{D}=\{x_i,y_i\}\) with $N$ observations and a model with parameters \(\boldsymbol{\theta}\) we want to find the best estimate for the true value for \(y\):</p>

\[\hat{y} = \theta_0 x^0 + \theta_1 x + \theta_2 x^2 + ... \theta_M x^M = \boldsymbol{\theta}^T\boldsymbol{\Phi} ~,\]

<p>where we defined \(\boldsymbol{\Phi} = (\Phi_0(x_i),...,\Phi_M(x_i))\) to be a \(N\times M\) matrix with \(\Phi_p(x_i)=x_i^p\) of and \(\hat{y}\) denotes the output of our model. Since all real world data is corrupted or distorted by noise coming from different sources (e.g. limitations in measurement tools), the true observations are pertubed with noise $\epsilon$, which is assumed to be a Gaussian with zero mean and variance \(\sigma^2\):</p>

\[y_i=\boldsymbol{\theta}^T\mathbf{x}_i + \epsilon_i \quad \textrm{with} \quad \epsilon_i \sim \mathcal{N}(0,\sigma^2) ~.\]

<p>Thus, we can model each point with a Gaussian distribution</p>

\[p(y_i|\Phi_i,\boldsymbol{\theta},\sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}\exp \left\{-\frac{1}{2\sigma^2}(y_i - \boldsymbol{\theta}^T\Phi_i)^2 \right\} ~.\]

<p>Assuming i.i.d. data points the probability of all points called likelihood factorizes:</p>

\[p(\mathbf{y}|\boldsymbol{\Phi},\boldsymbol{\theta}) = \prod_i p(y_i|\Phi_i,\boldsymbol{\theta}) = \sum_i \log p(y_i|\Phi_i,\boldsymbol{\theta}) ~,\]

<p>where \(\sigma^2\) is absorbed into \(\boldsymbol{\theta}\) making it a variable of our model. Derivating for \(\boldsymbol{\theta}\) and setting the derivative to 0 yields the maximum likelihood estimate (MLE). In contrast to MLE variational inference (VI) and Markov chain Monte Carlo (MCMC) sampling provide measures for certainty in the proposed parameters by making use of Bayes’ theroem:</p>

\[p(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y})= \frac{p(\mathbf{y}|\boldsymbol{\Phi},\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathbf{y}|\boldsymbol{\Phi})}~, \\\textrm{which speaks}\quad \textrm{posterior} = \frac{\textrm{likelihood } \times \textrm{ prior}}{\textrm{evidence}} ~.\]

<p>While the likelihood is the one from above, we introduce three new terms here: the posterior, prior, and evidence. The likelihood is multiplied by a prior, a distribution over \(\boldsymbol{\theta}\), that quantifies our believe in the model parameters prior to any training. We can also express zero prior knowledge by using a uniform distribution or a fairly wide Gaussian, when we assume our parameters to have Gaussian distributions. When we have computed the product of likelihood and prior, the evidence normalizes that product to obtain a valid probability distribution. The evidence can be seen as probability for seeing that particular data. After we have performed these computations, we obtain the posterior: the probability distribution of the parameters after seeing data.
For a new data point \((x_∗, y_*)\) the prediction of the model is obtained by considering
the predictions made using all possible parameter setting, weighted by their posterior
probability:</p>

\[p(y_*|\Phi(x_*),\boldsymbol{\Phi},\mathbf{y}) = \int p(y_*|\Phi(x_*),\boldsymbol{\theta})p(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y})d\boldsymbol{\theta} ~.\]

<p>Problematically this integral becomes intractable for even small models that are non-linear, so that other techniques such as VI and MCMC sampling must be employed. Here we compare both methods for the linear regression case starting with the exact approximation with Markov chain Monte Carlo sampling.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="p">.</span><span class="nb">set</span><span class="p">()</span>

<span class="n">matplotlib</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'text.usetex'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">matplotlib</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'text.latex.preamble'</span><span class="p">]</span> <span class="o">=</span> <span class="sa">r</span><span class="s">'\usepackage{bm}'</span>
</code></pre></div></div>

<h3 id="generating-example-data">Generating example data</h3>
<p>\(y = \theta_0 + \theta_1 x + \epsilon = -1 + x + \epsilon \quad \textrm{with} \quad \epsilon \sim \mathcal{N}(0,0.15)\) .</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">))]).</span><span class="n">transpose</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">weights</span><span class="p">[:,</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]).</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">no_samples</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">no_samples</span><span class="p">)</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">std_noise</span> <span class="o">=</span> <span class="mf">1.2</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="n">y_noisy</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">std_noise</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/compare_mcmc_vi_files/compare_mcmc_vi_3_0.png" alt="png" /></p>

<h3 id="markov-chain-monte-carlo-sampling">Markov Chain Monte Carlo Sampling</h3>

<p>One popular technique for approximating the intractable posterior is MCMC sampling, contrary to other methods it makes no assumption concerning the form of the distribution, such as wether it can be approximated by a multivariate Gaussian. They only assume the posterior</p>

\[p(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y})\]

<p>can be calculated up to normalization constant \(Z\) meaning</p>

\[p(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y})=\tilde{p}(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y})/Z~,\]

<p>where \(Z\) denotes the evidence in our case (<a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">Bishop 2006</a>).</p>

<p>In general, sampling methods try to find the expectation of some function \(\mathbf{f}_{\boldsymbol{\theta}}\) w.r.t. the posterior distribution for the model parameter:</p>

\[\mathbb{E}(\mathbf{f}) = \int \mathbf{f}_{\boldsymbol{\theta}}(\mathbf{x}_*)p(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y})d\boldsymbol{\theta} ~.\]

<p>The integral in above equation is approximated using Monte Carlo sampling:</p>

\[\mathbb{E}(\mathbf{f}) = \frac{1}{M} \sum_{i=1}^M \mathbf{f}_{\boldsymbol{\theta}_i}(\mathbf{x}_i) \quad \textrm{where} \quad \boldsymbol{\theta}_i \sim p(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y}) ~.\]

<p>Similar, the variance can be denoted by</p>

\[\textrm{Var}[\mathbf{f}] = \frac{1}{M} \mathbb{E}[(\mathbf{f}-\mathbb{E}[\mathbf{f}])^2] ~,\]

<p>if the generated samples from the posterior \(\boldsymbol{\theta}_i\) are independent. For complicated posterior distributions this is mostly impossible, but it still gives an unbiased estimate, if the number of generated samples is high enough (<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf">Neal 1996</a>).</p>

<p>To generate a set of dependent weights \(\boldsymbol{\theta}_i\) a Markov chain can be utilized that has
the posterior \(p(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y})\) as its equilibrium distribution. Markov Chains are a sequence of events, where the probability of one event depends only on the state of the previous one. So, one
samples from a proposal distribution \(q(\boldsymbol{\theta}|\boldsymbol{\theta}_i)\) and maintains a record of the current state \(\boldsymbol{\theta}_i\). A Markov chain is defined by giving an initial distribution for the
first state of the chain \(\boldsymbol{\theta}_1\) and a transition distribution for a new state \(\boldsymbol{\theta}_{i+1}\) following from the current state \(\boldsymbol{\theta}_i\). A stationary distribution q is established if the distribution
given by state \(\boldsymbol{\theta}_{i+1}\) is the same as with state \(\boldsymbol{\theta}_i\). If the drawn samples are dependent then early drawn samples need to be discarded, since they usually are not representatives
of the equilibrium distribution referred to as burn in phase. If the samples are dependent
the chain also needs much longer to reach its equilibrium distribution (<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf">Neal 1996</a>).</p>

<p>A popular algorithm for MCMC sampling is Metropolis-Hastings (<a href="https://bayes.wustl.edu/Manual/EquationOfState.pdf">Metropolis et al. 1953</a>, <a href="https://academic.oup.com/biomet/article-abstract/57/1/97/284580?redirectedFrom=fulltext">Hastings 1970</a>). The acceptance probability Ai at time step i is given by</p>

\[A_i(\boldsymbol{\theta}_*,\boldsymbol{\theta}_i) = \textrm{min} \left( 1, \frac{q(\boldsymbol{\theta}_i|\boldsymbol{\theta}_*)\tilde{p}(\boldsymbol{\theta}_*)}{q(\boldsymbol{\theta}_*|\boldsymbol{\theta}_i)\tilde{p}(\boldsymbol{\theta}_i)}  \right) ~,\]

<p>where \(\boldsymbol{\theta}_i\) denotes the current state and \(\boldsymbol{\theta}_*\) the drawn proposal state, \(\tilde{p}(\boldsymbol{\theta})\) is the prior of the model parameters. The normalization constants cancel out each other. After the acceptance probability is calculated, a random number \(r\) is drawn from a Uniform distribution \(r \sim \mathcal{U}(0,1)\). If \(A_i &gt; r\) the proposal state is accepted.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MCMC</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">burnin_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span> <span class="o">=</span> <span class="n">num_iter</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">burnin_iter</span> <span class="o">=</span> <span class="n">burnin_iter</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">start_params</span> <span class="o">=</span> <span class="n">start_params</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">proposal_fct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">scale_weights</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">scale_std_noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_weights</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="n">scale_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">scale_weights</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">params</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale_weights</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale_std_noise</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">metropolis_hastings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">chain</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">start_params</span><span class="p">)))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">chain</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">start_params</span>

        <span class="n">log_post</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
            <span class="n">proposal</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">proposal_fct</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">posterior_prob</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_post</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_post</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
            <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">posterior_prob</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">proposal</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">posterior_params</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">chain</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">burnin_iter</span><span class="p">:].</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">posterior_params</span>

    <span class="k">def</span> <span class="nf">posterior_pred</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">chain</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">burnin_iter</span><span class="p">:]])</span>
        <span class="k">return</span> <span class="n">y_pred</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y_pred</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">flatten</span><span class="p">()</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">prior_scale_weights</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">prior_scale_noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prior_scale_weights</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="n">prior_scale_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">prior_scale_weights</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
        <span class="n">log_prior_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">psw</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">psw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">prior_scale_weights</span><span class="p">)])</span>
        <span class="n">log_prior_noise</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">uniform</span><span class="p">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">prior_scale_noise</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_prior_weights</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">log_prior_noise</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">log_likelihoods</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">log_likelihoods</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_prior</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="variational-inference">Variational Inference</h2>

<p>Coming from the exact but time consuming approximation technique of MCMC we will now go on and revisit an approximate inference technique called Variational inference (VI). We will mainly follow the derivations of <a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">Bishop (2006)</a>. VI has its origin in the 18th century by the work of Euler, Lagrange and others on the calculus of variations, which works on functionals. In contrast to a function that takes a value as input and returns another value, a functional takes a function as input and returns a value. An example for this is the entropy:</p>

\[\textrm{H}[p]=-\int p(x) \log p(x) \, \mathrm{d}x ~,\]

<p>which takes as input a probability distribution and returns a value. The derivative of a function describes how much the output value changes as we make infinitesimal changes to the input value. Consequently, the derivative of a functional describes how much the output value changes, if we make infinitesimal changes to the function. Our goal is to find the function that minimizes the functional. Since many functions (such as neural networks) are very complex because of their high number of parameters, they lend themselves to approximation by restricting the range of functions over which the optimization is performed.</p>

<p>In the fully Bayesian treatment of VI all parameters are given prior distributions, where \(\boldsymbol{\theta}\) are our parameters (and all latent variables) and \(\mathbf{X}\) are our observed variables. Our probabilistic model is given by the joint distribution \(p(\mathbf{X},\boldsymbol{\theta})\). Our goal is to find approximations for the posterior the evidence from Bayes theorem. We do so by approximating our intractable posterior distribution \(p(\boldsymbol{\theta}\vert\mathbf{X},\mathbf{y})\) with a simpler distribution \(q(\boldsymbol{\theta})\) from a family of distributions \(\mathcal{Q}\) e.g. the multivariate Gaussian. The optimization objective is then given by the Kullback-Leibler (KL) divergence between our approximate and the true posterior:</p>

\[F(q):=\mathrm{KL}(q(\boldsymbol{\theta})||p(\boldsymbol{\theta}|\mathcal{D}))=\int q(\boldsymbol{\theta})\log \frac{q(\boldsymbol{\theta})}{p(\boldsymbol{\theta}|\mathcal{D})}\,\mathrm{d}\boldsymbol{\theta} \longrightarrow \underset{q(\boldsymbol{\theta}) \in \mathcal{Q}}{\min} ~.
\label{eq:vi_criterion}\]

<p>Although the KL divergence is not a true distance metric because of its asymmetry, it can be seen as one in this case. The KL divergence is analysed in more depth in this <a href="">post</a>. For now it is enough to now that it is only zero if and only if both distributions are equal. For all other cases it is always greater than zero.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">]])</span>
<span class="n">cov1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">cov</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">cov2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">cov</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">cov_vi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="p">[</span><span class="n">cov1</span><span class="p">,</span> <span class="n">cov2</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">n_std</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">ell_true</span> <span class="o">=</span> <span class="n">confidence_ellipse</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">n_std</span><span class="o">=</span><span class="n">n_std</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'#6A83B5'</span><span class="p">)</span>
        <span class="n">ell_vi</span> <span class="o">=</span> <span class="n">confidence_ellipse</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov_vi</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">n_std</span><span class="o">=</span><span class="n">n_std</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="p">[</span><span class="sa">r</span><span class="s">'$\mathrm{KL}(q(\bm{\theta})||p(\bm{\theta}))$'</span><span class="p">,</span>
                           <span class="sa">r</span><span class="s">'$\mathrm{KL}(p(\bm{\theta})||q(\bm{\theta}))$'</span><span class="p">]):</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\theta_0$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\theta_1$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="n">ell_true</span><span class="p">,</span> <span class="n">ell_vi</span><span class="p">],</span> <span class="p">[</span><span class="sa">r</span><span class="s">'$\mathcal{N}(\bm{\mu},\bm{\Sigma})$'</span><span class="p">,</span>
                                   <span class="sa">r</span><span class="s">'$\prod_i \mathcal{N}(\mu_i,\sigma_i)$'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/assets/imgs/compare_mcmc_vi_files/compare_mcmc_vi_7_0.png" alt="png" /></p>

<h3 id="bayesian-linear-regression-with-variational-inference">Bayesian Linear Regression with Variational Inference</h3>

<p>We perform Bayesian linear regression on the same model already used in the blog post about <a href="https://maltetoelle.github.io/linear/regression/2020/10/27/try.html">Bayesian linear regression</a> using Bayes theorem. Recall the likelihood for our observed target variables \(\mathbf{y}\) and the prior distribution for our model parameters \(\boldsymbol{\theta}\) are given by</p>

\[\begin{aligned}
p(\mathbf{y}|\boldsymbol{\theta}) &amp;= \prod_{n=1}^{N} \mathcal{N}\left( y_n | \boldsymbol{\theta}^T \Phi_n, \beta^{-1} \right) ~, \\
p(\boldsymbol{\theta}|\alpha) &amp;= \mathcal{N}\left( \mathbf{0},\alpha^{-1}\mathbf{I}_{M+1} \right) ~,
\end{aligned}\]

<p>with \(\Phi_n=\Phi(x_n)=(x_n^0,x_n^1,...,x_n^{M})^T\) where \(M\) denotes the degree of the fitted polynomial and \(\mathbf{I}_{M+1}\) denotes the identity matrix of size \(M+1\). We now introduce prior distributions over \(\alpha\) and \(\beta\). The conjugate prior for Gaussian distributions is the Wishart distribution or in the one dimensional case the Gamma distribution:</p>

\[\begin{aligned}
p(\alpha) &amp;= \textrm{Gam}(\alpha|a_0,b_0) ~, \\
p(\beta) &amp;= \textrm{Gam}(\beta|c_0,d_0) ~.
\end{aligned}\]

<p>The definition of the Gamma and log Gamma distribution can be found in the appendix at the end of the post. Thus, the joint distribution of all the variables is given by</p>

\[p(\mathbf{y},\boldsymbol{\theta},\alpha,\beta) = p(\mathbf{y}|\boldsymbol{\theta},\beta) p(\boldsymbol{\theta}|\alpha) p(\beta) p(\alpha) ~.\]

<p>By using the mean field approximation the approximation of the posterior \(p(\boldsymbol{\theta},\alpha,\beta)\) is given by the factorization</p>

\[q(\boldsymbol{\theta},\alpha,\beta)=q(\boldsymbol{\theta})q(\alpha)q(\beta) ~.\]

<p>We can find the optimal variational parameters for each of the above distributions by making use of Eq. (4). For each factor, we take the log of the joint distribution over all variables and then average w.r.t. to those variables not in the factor.</p>

<h4 id="variational-density-for-alpha">Variational density for \(\alpha\)</h4>

<p>The log of our optimal variational density \(q^*(\alpha)\) is given by</p>

\[\begin{aligned}
\log q^*(\alpha) &amp;= \log p(\alpha) + \mathbb{E}\left[ \log p(\boldsymbol{\theta}|\alpha) \right] + \mathcal{Z} \\
&amp;= (a_0 - 1) \log a - b_0 \alpha + \frac{M}{2} \log \alpha - \frac{\alpha}{2} \mathbb{E} \left[ \boldsymbol{\theta}^T \boldsymbol{\theta} \right] + \mathcal{Z} ~.
\end{aligned}\]

<p>We notice the above as the parameters of a log Gamma distribution, so, we can perform coefficient comparison and find the optimal variational distribution for \(\alpha\) to be</p>

\[q^*(\alpha) = \textrm{Gam}(\alpha|a_N,b_N) ~,\]

<p>where</p>

\[\begin{aligned}
a_N &amp;= a_0 + \frac{M}{2} ~, \\
b_N &amp;= b_0 + \frac{1}{2}\mathbb{E}\left[ \boldsymbol{\theta}^T\boldsymbol{\theta} \right] ~.
\end{aligned}\]

<h4 id="variational-density-for-boldsymboltheta">Variational density for \(\boldsymbol{\theta}\)</h4>

<p>Similarly, the log of our optimal variational density \(q^*(\theta)\) is given by</p>

\[\begin{aligned}
\log q^*(\boldsymbol{\theta}) &amp;= \mathbb{E}_{\beta}\left[ \log p(\mathbf{y}|\boldsymbol{\theta},\beta) \right] + \mathbb{E}_{\alpha}\left[ p(\boldsymbol{\theta}|\alpha) \right] + \mathcal{Z} \\
&amp;\propto -\frac{\mathbb{E}_{\beta}}{2} \sum_{n=1}^{N}\left( y_n - \boldsymbol{\theta}^T\Phi_n \right)^2 - \frac{\mathbb{E}_{\alpha}}{2} \boldsymbol{\theta}^T\boldsymbol{\theta} + \mathcal{Z} \\
&amp;\propto -\frac{\mathbb{E}_{\beta}}{2} \sum_{n=1}^{N} \left\{-2\boldsymbol{\theta}^T\Phi_n y_n + \left(\boldsymbol{\theta}^T\Phi_n\right)^2 \right\} - \frac{\mathbb{E}_{\alpha}}{2} \boldsymbol{\theta}^T\boldsymbol{\theta} + \mathcal{Z} \\
&amp;= -\frac{1}{2} \boldsymbol{\theta}^T \left\{ \mathbb{E}_{\beta} \boldsymbol{\Phi}^T\boldsymbol{\Phi} + \mathbb{E}_{\alpha}\mathbf{I} \right\} \boldsymbol{\theta} + \mathbb{E}_{\beta}\boldsymbol{\theta}^T\boldsymbol{\Phi}^T\mathbf{y} + \mathcal{Z} ~.
\end{aligned}\]

<p>We recognize this as a log normal distribution and find our coefficients to be</p>

\[q^*(\boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{\theta}|\mathbf{m}_N,\mathbf{S}_N) ~,\]

<p>where</p>

\[\begin{aligned}
\mathbf{m}_N &amp;= \mathbb{E}_{\beta} \mathbf{S}_N \boldsymbol{\Phi}^T \mathbf{y} ~, \\
\mathbf{S}_N &amp;= \left( \mathbb{E}_{\beta}\boldsymbol{\Phi}^T\boldsymbol{\Phi} + \mathbb{E}_{\alpha} \mathbf{I} \right)^{-1} ~.
\end{aligned}\]

<h4 id="variational-density-for-beta">Variational Density for \(\beta\)</h4>

<p>As for \(\alpha\) and \(\boldsymbol{\theta}\), the optimal variational density \(q^*(\beta)\) is given by</p>

\[\begin{aligned}
\log q^*(\beta) &amp;= \log p(\beta) + \mathbb{E}_{\boldsymbol{\theta}}\left[ \log p(\mathbf{y}|\boldsymbol{\theta},\beta) \right] + \mathcal{Z} \\
&amp;\propto (c_0 - 1)\log \beta - d_0\beta + \frac{N}{2} \log \beta - \frac{\beta}{2} \mathbb{E} \left[ \sum_{n=1}^{N} \left( y_n - \boldsymbol{\theta}^T\Phi_n \right)^2 \right] + \mathcal{Z} \\
&amp;= \left( \frac{N}{2} + c_0 - 1 \right)\log\beta - d_0\beta - \frac{\beta}{2} \mathbb{E}\left[ \mathbf{y}^T\mathbf{y} -2\boldsymbol{\theta}\boldsymbol{\Phi} \mathbf{y} + \boldsymbol{\theta}^T\boldsymbol{\Phi}^T\boldsymbol{\Phi}\boldsymbol{\theta} \right] + \mathcal{Z} \\
&amp;= \left( \frac{N}{2} + c_0 - 1 \right)\log\beta -\beta \left\{ d_0 + \frac{1}{2}\mathrm{Tr}\left[ \boldsymbol{\Phi}^T\boldsymbol{\Phi}\, \mathbb{E}\left[ \boldsymbol{\theta}^T\boldsymbol{\theta} \right] \right] - \mathbb{E}\left[ \boldsymbol{\theta}^T \right] \boldsymbol{\Phi}\mathbf{y} + \frac{1}{2}\mathbf{y}^T\mathbf{y} \right\} + \mathcal{Z} \\
&amp;= \left( \frac{N}{2} + c_0 - 1 \right)\log\beta -\beta \left\{ d_0 + \frac{1}{2}\mathrm{Tr}\left[ \boldsymbol{\Phi}^T\boldsymbol{\Phi}\left( \mathbf{m}_N\mathbf{m}_N^T + \mathbf{S}_N \right) \right] - \mathbf{y}^T\boldsymbol{\Phi}\mathbf{m}_N + \frac{1}{2}\mathbf{y}^T\mathbf{y} \right\} + \mathcal{Z} \\
&amp;= \left( \frac{N}{2} + c_0 - 1 \right)\log\beta -\beta \left\{ d_0 + \frac{1}{2}\mathrm{Tr}\left[ \boldsymbol{\Phi}^T\boldsymbol{\Phi}\mathbf{S}_N \right] + \frac{1}{2}\mathbf{m}_N^T\boldsymbol{\Phi}^T\boldsymbol{\Phi}\mathbf{m}_N - \mathbf{y}^T\boldsymbol{\Phi}\mathbf{m}_N + \frac{1}{2}\mathbf{y}^T\mathbf{y} \right\} + \mathcal{Z} \\
&amp;= \left( \frac{N}{2} + c_0 - 1 \right)\log\beta -\frac{\beta}{2} \left\{ 2d_0 + \mathrm{Tr}\left[ \boldsymbol{\Phi}^T\boldsymbol{\Phi}\mathbf{S}_N \right] + \sum_{n=1}^{N}\left( y_n - \mathbf{m}_N^T\Phi_n \right)^2 \right\} + \mathcal{Z} ~.
\end{aligned}\]

<p>We again recognize this as the coefficients of a log Gamma distribution</p>

\[q^*(\beta) = \textrm{Gam}(\beta|c_N,d_N) ~,\]

<p>where</p>

\[\begin{aligned}
c_N &amp;= \frac{N}{2} + c_0 ~, \\
d_N &amp;= d_0 + \frac{1}{2} \left\{ \mathrm{Tr}\left[ \boldsymbol{\Phi}^T\boldsymbol{\Phi}\mathbf{S}_N \right] + \sum_{n=1}^{N} \left( y_n - \mathbf{m}_N^T\Phi_n \right)^2 \right\} ~.
\end{aligned}\]

<h4 id="estimating-the-missing-moments">Estimating the Missing Moments</h4>

<p>The missing moments of the Gamma distributions for \(\alpha\) and \(\beta\) can be easily estimated from the definition of the distribution.</p>

\[\begin{aligned}
\mathbb{E}[\alpha] &amp;= \frac{a_N}{b_N} \\
&amp;= \frac{a_0 + \frac{M}{2}}{b_0 + \frac{1}{2}\mathbb{E}\left[ \boldsymbol{\theta}^T\boldsymbol{\theta} \right]}\\
&amp;= \frac{a_0 + \frac{M}{2}}{b_0 + \frac{1}{2}\mathbf{m}_N^T\mathbf{m}_N + \textrm{Tr}(\mathbf{S}_N)} \quad \textrm{with} \quad \mathbb{E}\left[ \boldsymbol{\theta}\boldsymbol{\theta}^T \right] = \mathbf{m}_N\mathbf{m}_N^T + \mathbf{S}_N ~, \\
\mathbb{E}[\beta] &amp;= \frac{c_N}{d_N} \\
&amp;= \frac{\frac{N}{2} + c_0}{d_0 + \frac{1}{2} \left\{ \mathrm{Tr}\left[ \boldsymbol{\Phi}^T\boldsymbol{\Phi}\mathbf{S}_N \right] + \sum_{n=1}^{N} \left( y_n - \mathbf{m}_N^T\Phi_n \right)^2 \right\}}
\end{aligned}\]

<p>VI for linear regression is performed by cyclically estimating the parameters \(a_N\), \(b_N\), \(c_N\), \(d_N\), \(\mathbf{m}_N\), and \(\mathbf{S}_N\) with the corresponding update formulas. Which can be done in a small number of lines in code compared to the long derivations above. But before we take a look at the code, we must examine the predictive distribution of the model.</p>

<h4 id="predictive-distribution">Predictive Distribution</h4>

<p>The predictive distribution for new data points \((\mathbf{x}_*,\mathbf{y}_*)\) can easily be evaluated using the Gaussian variational posterior for the parameters</p>

\[\begin{aligned}
p(\mathbf{y}_*|\mathbf{x}_*,\mathbf{y}) &amp;= \int p(\mathbf{y}_*|\mathbf{x}_*,\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathbf{y})\,d\boldsymbol{\theta} \\
&amp;= \int p(\mathbf{y}_*|\mathbf{x}_*,\boldsymbol{\theta})q(\boldsymbol{\theta})\,d\boldsymbol{\theta} \\
&amp;= \int\mathcal{N}\left( \mathbf{y}_*|\boldsymbol{\theta}^T\boldsymbol{\Phi}(\mathbf{x}_*), \mathbb{E}[\beta] \right) \mathcal{N}\left( \boldsymbol{\theta} | \mathbf{m}_N,\mathbf{S}_N \right)\,d\boldsymbol{\theta} \\
&amp;= \mathcal{N}\left( \mathbf{m}_N^T\boldsymbol{\Phi}(\mathbf{x}_*),\sigma^2(\mathbf{x}_*) \right) \quad \textrm{with} \quad \sigma^2(\mathbf{x}_*) = \frac{1}{\mathbb{E}[\beta]} + \boldsymbol{\Phi}(\mathbf{x}_*)^T \mathbf{S}_N \boldsymbol{\Phi}(\mathbf{x}_*) ~.
\end{aligned}\]

<p>In the above we incorporate the noise inherent in the observations in our prediction. Contrary to that we will sample our weights from the estimated mean and covariance and treat the noise in the observation as not be a part of the model. Our prediction then becomes</p>

\[\begin{gathered}
\boldsymbol{\theta}_i \sim \mathcal{N}(\boldsymbol{\theta}|\mathbf{m}_N,\mathbf{S}_N) ~,\\
y_i = \boldsymbol{\theta}_i^T\boldsymbol{\Phi} ~.
\end{gathered}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VI</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">start_alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">start_beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">a0</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">b0</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">c0</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">d0</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">):</span>

        <span class="c1"># initializing all with 0 corresponds to an infinitely wide prior
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">a0</span> <span class="o">=</span> <span class="n">a0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b0</span> <span class="o">=</span> <span class="n">b0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">c0</span> <span class="o">=</span> <span class="n">c0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d0</span> <span class="o">=</span> <span class="n">d0</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">start_alpha</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">start_beta</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span> <span class="o">=</span> <span class="n">num_iter</span>

    <span class="k">def</span> <span class="nf">training</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">pol_grad</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">pol_grad</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">D</span><span class="p">)]).</span><span class="n">transpose</span><span class="p">()</span>

        <span class="n">aN</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">a0</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">D</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">cN</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">c0</span> <span class="o">+</span> <span class="n">N</span><span class="o">/</span><span class="mi">2</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span><span class="p">)):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">SN</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">D</span><span class="p">))</span>

            <span class="bp">self</span><span class="p">.</span><span class="n">mN</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">SN</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>

            <span class="n">dN</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">d0</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">mN</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">SN</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">cN</span><span class="o">/</span><span class="n">dN</span>

            <span class="n">bN</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">b0</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">mN</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">mN</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">SN</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">aN</span><span class="o">/</span><span class="n">bN</span>

        <span class="c1"># estimated variance in the training data
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">x_var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="bp">self</span><span class="p">.</span><span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="p">.</span><span class="n">SN</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span>

        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">mN</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">SN</span>

    <span class="k">def</span> <span class="nf">posterior_pred</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_pred</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_pred</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">D</span><span class="p">)]).</span><span class="n">transpose</span><span class="p">()</span>

        <span class="c1">#mean = X @ self.mN
</span>        <span class="c1">#var = np.array([1/self.beta + x @ self.SN @ x.T for x in X])
</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
            <span class="c1">#pred = np.random.normal(mean, np.sqrt(var))
</span>            <span class="n">weight_sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mN</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">SN</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">weight_sample</span><span class="p">[:,</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
            <span class="n">preds</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">preds</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">preds</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">flatten</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">start_params_mcmc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">start_params</span><span class="o">=</span><span class="n">start_params_mcmc</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="n">posterior_params_mcmc</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">metropolis_hastings</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 9999/9999 [00:16&lt;00:00, 602.29it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vi</span> <span class="o">=</span> <span class="n">VI</span><span class="p">(</span><span class="n">num_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">w_mean_vi</span><span class="p">,</span> <span class="n">w_cov_vi</span> <span class="o">=</span> <span class="n">vi</span><span class="p">.</span><span class="n">training</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 4991.97it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">y_true_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

<span class="n">y_pred_mcmc_mean</span><span class="p">,</span> <span class="n">y_pred_mcmc_std</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">posterior_pred</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span>
<span class="n">y_pred_vi_mean</span><span class="p">,</span> <span class="n">y_pred_vi_std</span> <span class="o">=</span> <span class="n">vi</span><span class="p">.</span><span class="n">posterior_pred</span><span class="p">(</span><span class="n">x_pred</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_true_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_pred_vi_mean</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'VI'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_pred_vi_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_pred_vi_std</span><span class="p">,</span> <span class="n">y_pred_vi_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_pred_vi_std</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_pred_mcmc_mean</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'MCMC'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_pred_mcmc_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_pred_mcmc_std</span><span class="p">,</span> <span class="n">y_pred_mcmc_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_pred_mcmc_std</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$y$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'reg_mcmc_vi.pdf'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/compare_mcmc_vi_files/compare_mcmc_vi_12_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">chain</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">):</span>
    <span class="c1"># mcmc
</span>    <span class="n">samples</span> <span class="o">=</span> <span class="n">mcmc</span><span class="p">.</span><span class="n">chain</span><span class="p">[</span><span class="n">mcmc</span><span class="p">.</span><span class="n">burnin_iter</span><span class="o">-</span><span class="mi">1</span><span class="p">:,</span><span class="n">i</span><span class="p">]</span>
    <span class="n">x_w_viz</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">samples</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">samples</span><span class="p">.</span><span class="nb">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>

    <span class="n">mu_mcmc</span><span class="p">,</span> <span class="n">std_mcmc</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">axs</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="c1"># mcmc
</span>        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_w_viz</span><span class="p">,</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_w_viz</span><span class="p">,</span> <span class="n">mu_mcmc</span><span class="p">,</span> <span class="n">std_mcmc</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'mcmc'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">)</span>
        <span class="c1"># vi
</span>        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_w_viz</span><span class="p">,</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_w_viz</span><span class="p">,</span> <span class="n">vi</span><span class="p">.</span><span class="n">mN</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">vi</span><span class="p">.</span><span class="n">SN</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]),</span> <span class="n">label</span><span class="o">=</span><span class="s">'vi'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">title</span><span class="p">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">r</span><span class="s">'posterior -- $w_%d$'</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">vi_std_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vi</span><span class="p">.</span><span class="n">x_var</span><span class="p">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="n">vi_std_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vi</span><span class="p">.</span><span class="n">x_var</span><span class="p">).</span><span class="n">std</span><span class="p">()</span>
        <span class="n">new_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">mu_mcmc</span><span class="o">-</span><span class="mf">3.5</span><span class="o">*</span><span class="n">std_mcmc</span><span class="p">,</span> <span class="n">mu_mcmc</span><span class="o">+</span><span class="mf">3.5</span><span class="o">*</span><span class="n">std_mcmc</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="c1"># mcmc
</span>        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="n">mu_mcmc</span><span class="p">,</span> <span class="n">std_mcmc</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'mcmc'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">)</span>
        <span class="c1"># vi
</span>        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">new_x</span><span class="p">,</span> <span class="n">vi_std_mean</span><span class="p">,</span> <span class="n">vi_std_std</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">'vi'</span><span class="p">)</span>

        <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">std_noise</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">title</span><span class="p">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">r</span><span class="s">'posterior -- $\sigma_{noise}$'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/compare_mcmc_vi_files/compare_mcmc_vi_13_0.png" alt="png" /></p>

<h2 id="appendix">Appendix</h2>

<h3 id="log-gamma-distribution">(Log) Gamma distribution</h3>

\[\begin{aligned}
\textrm{Gam}(x|a,b) &amp;= \frac{1}{\Gamma (a)} b^{a} x^{a-1} e^{-bx} \\
&amp;= \frac{1}{(a-1)!} b^{a} x^{a-1} e^{-bx} \\
\log \textrm{Gam}(x|a,b) &amp;= \underbrace{-\log\left((a-1)!\right) + a\log b}_{\textrm{const. w.r.t. }x} + (a-1)\log x - bx
\end{aligned}\]

<h3 id="code-for-drawing-confidence-ellipse">Code for Drawing Confidence Ellipse</h3>

<p>From official matplotlib <a href="https://matplotlib.org/devdocs/gallery/statistics/confidence_ellipse.html">documentation</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib.patches</span> <span class="kn">import</span> <span class="n">Ellipse</span>
<span class="kn">import</span> <span class="nn">matplotlib.transforms</span> <span class="k">as</span> <span class="n">transforms</span>

<span class="c1">#def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', edgecolor='black', **kwargs):
</span><span class="k">def</span> <span class="nf">confidence_ellipse</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">n_std</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'none'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="s">"""
    Create a plot of the covariance confidence ellipse of *x* and *y*.
    From: https://matplotlib.org/devdocs/gallery/statistics/confidence_ellipse.html

    Parameters
    ----------
    x, y : array-like, shape (n, )
        Input data.

    ax : matplotlib.axes.Axes
        The axes object to draw the ellipse into.

    n_std : float
        The number of standard deviations to determine the ellipse's radiuses.

    **kwargs
        Forwarded to `~matplotlib.patches.Ellipse`

    Returns
    -------
    matplotlib.patches.Ellipse
    """</span>
    <span class="c1">#if x.size != y.size:
</span>    <span class="c1">#    raise ValueError("x and y must be the same size")
</span>
    <span class="c1">#cov = np.cov(x, y)
</span>    <span class="n">pearson</span> <span class="o">=</span> <span class="n">cov</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cov</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">cov</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="c1"># Using a special case to obtain the eigenvalues of this
</span>    <span class="c1"># two-dimensionl dataset.
</span>    <span class="n">ell_radius_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">pearson</span><span class="p">)</span>
    <span class="n">ell_radius_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pearson</span><span class="p">)</span>
    <span class="n">ellipse</span> <span class="o">=</span> <span class="n">Ellipse</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="n">ell_radius_x</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">ell_radius_y</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
                      <span class="n">facecolor</span><span class="o">=</span><span class="n">facecolor</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">edgecolor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Calculating the stdandard deviation of x from
</span>    <span class="c1"># the squareroot of the variance and multiplying
</span>    <span class="c1"># with the given number of standard deviations.
</span>    <span class="n">scale_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cov</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">n_std</span>
    <span class="n">mean_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># calculating the stdandard deviation of y ...
</span>    <span class="n">scale_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cov</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">n_std</span>
    <span class="n">mean_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">transf</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Affine2D</span><span class="p">()</span> \
        <span class="p">.</span><span class="n">rotate_deg</span><span class="p">(</span><span class="mi">45</span><span class="p">)</span> \
        <span class="p">.</span><span class="n">scale</span><span class="p">(</span><span class="n">scale_x</span><span class="p">,</span> <span class="n">scale_y</span><span class="p">)</span> \
        <span class="p">.</span><span class="n">translate</span><span class="p">(</span><span class="n">mean_x</span><span class="p">,</span> <span class="n">mean_y</span><span class="p">)</span>

    <span class="n">ellipse</span><span class="p">.</span><span class="n">set_transform</span><span class="p">(</span><span class="n">transf</span> <span class="o">+</span> <span class="n">ax</span><span class="p">.</span><span class="n">transData</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ax</span><span class="p">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">ellipse</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name>Malte Tölle</name><email>malte.toelle@med.uni-heidelberg.de</email></author><category term="linear" /><category term="regression" /><category term="vi" /><category term="mcmc" /><summary type="html"><![CDATA[Here you can fing more in depth information about the topic of linear regression in general. You might want to check it out, if you are unfamiliar with it. Here we will only review the most basic intuitions. In this post we will revisit two methods for approximating the intractable posterior in Bayesian inference, namely variational inference (VI) and Markov chain Monte Carlo (MCMC) sampling. While the latter is able to approximate the posterior exactly, it takes longer to converge. This can be eliminated with variational inference at the cost of no exact approximation. But more to come.]]></summary></entry><entry><title type="html">(Bayesian) Linear Regression</title><link href="http://0.0.0.0:4000/linear/regression/bayesian_linear_regression/" rel="alternate" type="text/html" title="(Bayesian) Linear Regression" /><published>2022-10-27T11:44:44+00:00</published><updated>2022-10-27T11:44:44+00:00</updated><id>http://0.0.0.0:4000/linear/regression/bayesian_linear_regression</id><content type="html" xml:base="http://0.0.0.0:4000/linear/regression/bayesian_linear_regression/"><![CDATA[<p>For an introduction to deep learning it is generally best to start with the simplest example of linear regression. In linear regression we want to obtain a model that best describes the correlation between input data points \(\mathbf{x}\) and output data points \(\mathbf{y}\) with some model parameters. For the sake of visualization our output dimensionality is one and our input dimensionality is \(D\):</p>

\[\hat{\mathbf{y}}_i = \theta_0 + \theta_1 x_1 + ... + \theta_D x_D ~,\]

<p>where \(\hat{\mathbf{y}}\) is the model’s prediction and \(\boldsymbol{\theta}=(\theta_0,...,\theta_D)^T\) denotes the vector of model parameters. By defining \(x_0=1\) we can rewrite the problem in a more compact form:</p>

\[\hat{\mathbf{y}}_i=\boldsymbol{\theta}^T \mathbf{x}_i ~.\]

<p>Since no process in the real world is noise free (e.g. because of limitations in measurement tools), we need to take this corruption into account:</p>

\[y_i=\boldsymbol{\theta}^T\mathbf{x}_i + \epsilon_i \quad \textrm{with} \quad \epsilon_i \sim \mathcal{N}(0,\sigma^2) ~.\]

<p>To generate the example data we must first import the necessary packages.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="p">.</span><span class="nb">set</span><span class="p">()</span>

<span class="n">matplotlib</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'text.usetex'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">matplotlib</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'text.latex.preamble'</span><span class="p">]</span> <span class="o">=</span> <span class="sa">r</span><span class="s">'\usepackage{bm}'</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="generating-example-data">Generating example data</h3>
<p>\(y = \theta_0 + \theta_1 x + \epsilon = -1 + x + \epsilon \quad \textrm{with} \quad \epsilon \sim \mathcal{N}(0,0.15)\) .</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">pol_grad</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="n">no_test_pts</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">x</span><span class="p">[:,</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]))</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="p">(</span><span class="n">_x</span> <span class="o">@</span> <span class="n">weights</span><span class="p">[:,</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]).</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">y_noisy</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">))</span>
<span class="n">y_noisy</span> <span class="o">=</span> <span class="n">y_noisy</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">x_train</span> <span class="o">=</span> <span class="n">y_noisy</span><span class="p">[:</span><span class="o">-</span><span class="n">no_test_pts</span><span class="p">],</span> <span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="n">no_test_pts</span><span class="p">]</span>
<span class="n">y_test</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">=</span> <span class="n">y_noisy</span><span class="p">[</span><span class="o">-</span><span class="n">no_test_pts</span><span class="p">:],</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="n">no_test_pts</span><span class="p">:]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$y$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/linear_regression_files/linear_regression_3_0.png" alt="png" /></p>

<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>

<p>Since we assume our noise to be Gaussian, we can model each output value with a Gaussian distribution:</p>

\[\begin{aligned}
p(y_i|\mathbf{x}_i,\boldsymbol{\theta},\sigma^2) &amp;= \mathcal{N}(\boldsymbol{\theta}^T \mathbf{x}_i,\sigma^2) \\
    &amp;= \frac{1}{\sigma \sqrt{2\pi}} \exp\left\{ -\frac{1}{2\sigma^2} \left( y_i - \boldsymbol{\theta}^T \mathbf{x}_i \right)^2 \right\} ~.
\end{aligned}\]

<p>\(p(y_i\vert\mathbf{x}_i,\boldsymbol{\theta},\sigma^2)\) is known as likelihood. It measures how well our model with parameters \(\boldsymbol{\theta}\) and variance \(\sigma^2\) is to generate the observed data. If we further assume independent and identically distributed (i.i.d.) data, the likelihood for all data points factorizes:</p>

\[p(\mathbf{Y}|\mathbf{X},\boldsymbol{\theta},\sigma^2) = \prod_{i=1}^{N} p(y_i|\mathbf{x}_i,\boldsymbol{\theta},\sigma^2) ~,\]

<p>with \(\mathbf{Y}=(y_1,...,y_N)^T\) and each row of the \(N \times D\) matrix \(\mathbf{X}\) consists of an observation of the feature vector \(\mathbf{x}_i\). Since we have one-dimensional output data, we will denote the dataset outputs with \(\mathbf{y}\) here, since each data point is one-dimensional and we, thus, have a vector. The above equation is referred to as likelihood function \(\mathcal{L}(\boldsymbol{\theta})\). For more complex models the observation usually is not trivial and requires techniques such as gradient descent, but for linear regression the solution can be found analytically by transforming the product into a sum by taking the logarithm:</p>

\[\begin{aligned}
    %\left[\hat{\boldsymbol{\theta}}, \hat{\sigma}^2\right] &amp;=
    \displaystyle\textrm{arg max}_{\theta, \sigma^2} \sum_{n=1}^{N} \log p\left(y_i|\mathbf{x}_i,\boldsymbol{\theta},\sigma^2\right)
    &amp;= \displaystyle\textrm{arg max}_{\theta, \sigma^2} \sum_{n=1}^{N} \log \frac{1}{\sigma \sqrt{2\pi}} \exp\left\{ -\frac{1}{2\sigma^2} \left( y_i - \boldsymbol{\theta}^T \mathbf{x}_i \right)^2 \right\} \\
    &amp;= \displaystyle\textrm{arg max}_{\theta, \sigma^2} \sum_{n=1}^{N} \log \frac{1}{\sigma\sqrt{2\pi}} - \sum_{n=1}^{N} \frac{1}{2\sigma^2} \left(y_i - \boldsymbol{\theta}^T \mathbf{x}_i\right)^2 \\
    &amp;= \displaystyle\textrm{arg max}_{\theta, \sigma^2} - N \log \sigma\sqrt{2\pi} - \frac{1}{2\sigma^2} \sum_{n=1}^{N} \left(y_i - \boldsymbol{\theta}^T \mathbf{x}_i\right)^2 \\
    &amp;= \displaystyle\textrm{arg max}_{\theta, \sigma^2} -N \log \sigma \sqrt{2\pi} - \frac{1}{2\sigma^2} \left(\mathbf{y} - \mathbf{X}\boldsymbol{\theta})^T (\mathbf{y} - \mathbf{X}\boldsymbol{\theta}\right) ~.
    \end{aligned}\]

<p>Since the likelihood function is convex, when the error is assumed to be Gaussian distributed, the optimal solutions can be found by taking the derivative with respect to \(\boldsymbol{\theta}\) and \(\sigma^2\) and equating to zero:</p>

\[\begin{gathered}
    \frac{\partial \log \mathcal{L}}{\partial\boldsymbol{\theta}} = -\frac{1}{\sigma^2}\left(\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}-\mathbf{X}^T\mathbf{y}\right)=0 ~,\\
    \frac{\partial\log\mathcal{L}}{\partial\sigma^2} = -\frac{N}{\sigma^2} + \frac{1}{(\sigma^2)^2} \left(\mathbf{y} - \mathbf{X}\boldsymbol{\theta}\right)^T \left(\mathbf{y} - \mathbf{X}\boldsymbol{\theta}\right) = 0 ~,\\
    \hat{\boldsymbol{\theta}} = \left( \mathbf{X}^T\mathbf{X}\right)^{-1} \mathbf{X}^T \mathbf{y} ~,\;\;\;\; \hat{\sigma}^2 = \frac{1}{N}\left(\mathbf{y} - \mathbf{X}\boldsymbol{\theta}\right)^T \left(\mathbf{y} - \mathbf{X}\boldsymbol{\theta}\right) ~.
    \end{gathered}\]

<p>In order to better fit the data the input features are transformed using basis functions \(\phi_p(x)=x^p\). The new features are gathered in the design matrix \(\boldsymbol{\Phi}\) of size \(N \times M\) where each row is the feature vector obtained by transforming one observation, \(\boldsymbol{\phi}_i=\left( \phi_1(x_i),...,\phi_M(x_i) \right)\). The basic equations from above are unchanged, \(\mathbf{X}\) is just substituted by \(\boldsymbol{\Phi}\). More simplistic, the model is then given by:</p>

\[\hat{y}_i = \theta_0 + \theta_1 x_i + \theta_2 x_i^2 + ... + \theta_M x_i^M ~.\]

<p>If we solve for the model parameters in the described manner, the solution is referred to as maximum likelihood estimate (MLE).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ys</span><span class="p">,</span> <span class="n">nlls</span><span class="p">,</span> <span class="n">test_mses</span><span class="p">,</span> <span class="n">covs</span><span class="p">,</span> <span class="n">variances</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="n">pol_grads</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="n">max_grad</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">_pol_grads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_grad</span><span class="p">)])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">_pol_grads</span><span class="p">:</span><span class="c1">#range(200, 202):
</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_train</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">)]).</span><span class="n">transpose</span><span class="p">()</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">)]).</span><span class="n">transpose</span><span class="p">()</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="n">y_train</span>

    <span class="n">mean</span> <span class="o">=</span> <span class="n">X_test</span> <span class="o">@</span> <span class="n">w</span>
    <span class="n">var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">).</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">)</span>

    <span class="n">cov</span> <span class="o">=</span> <span class="n">var</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>

    <span class="n">nll</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">var</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">).</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">)</span>

    <span class="n">train_mse</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span> <span class="o">*</span> <span class="p">((</span><span class="n">y_train</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">test_mse</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">no_test_pts</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">y_test</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">-</span> <span class="n">mean</span><span class="p">[</span><span class="o">-</span><span class="n">no_test_pts</span><span class="p">:])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>

    <span class="n">ys</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
    <span class="n">nlls</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nll</span><span class="p">)</span>
    <span class="n">test_mses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_mse</span><span class="p">)</span>
    <span class="n">covs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="n">variances</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="c1">#idx_gaussian = np.random.randint(0, len(x_train))
</span><span class="n">idx_gaussian</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">pt</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_train</span><span class="p">[</span><span class="n">idx_gaussian</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">idx_gaussian</span><span class="p">]]</span>

<span class="n">ax1</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">'$x_{train}$'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">'$x_{test}$'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pol_grads</span><span class="p">:</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">_pol_grads</span> <span class="o">==</span> <span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">idx_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">_pol_grads</span> <span class="o">==</span> <span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[</span><span class="n">idx_y</span><span class="p">]</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">_y</span><span class="p">[</span><span class="n">idx_y</span><span class="p">]</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variances</span><span class="p">[</span><span class="n">idx_y</span><span class="p">])</span>
    <span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="n">ax1</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'#7F73AF'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'#7F73AF'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y_true</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$y$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.6</span><span class="p">,</span><span class="mf">1.6</span><span class="p">])</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">pt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'#7F73AF'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$y$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$p(y|x,\bm{\theta},\sigma^2)$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>

<span class="n">ax3</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">nlls</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">nlls</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'degree of polynomial'</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$-\log p(\mathbf{y}|\boldsymbol{\Phi},\hat{\boldsymbol{\theta}},\hat{\sigma}^2)$'</span><span class="p">,</span>
                      <span class="n">color</span><span class="o">=</span><span class="s">'#748ABA'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">nlls</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">ax31</span> <span class="o">=</span> <span class="n">ax3</span><span class="p">.</span><span class="n">twinx</span><span class="p">()</span>
<span class="n">ax31</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_mses</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">test_mses</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">)</span>
<span class="n">ax31</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\textrm{MSE}(y_{test}, \hat{y}_{test})$'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">ax31</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/linear_regression_files/linear_regression_6_0.png" alt="png" /></p>

<h2 id="maximum-a-posterior">Maximum A Posterior</h2>

<p>The Maximum Likelihood Estimation described in the previous chapter is also called frequentist approach. Even with just two data points the model would give one solution as the only true one. Thus, MLE posses the disadvantage of being overconfident in its prediction. Remedy comes in the form of the Bayes Theorem, which provides the possibility of including some prior believe in the model prior of seeing any data. Given a data set \(\mathcal{D}=\{\mathbf{X}, \mathbf{Y}\}\) and some model parameter \(\boldsymbol{\theta}\) Bayes Theorem is given by:</p>

\[\begin{gathered}    p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y},\sigma^2)=\frac{p(\mathbf{Y}|\mathbf{X},\boldsymbol{\theta},\sigma^2)p(\boldsymbol{\theta})}{p(\mathbf{Y}|\mathbf{X},\sigma^2)} = \frac{p(\mathbf{Y}|\mathbf{X},\boldsymbol{\theta},\sigma^2)p(\boldsymbol{\theta})}{\int p(\mathbf{Y}|\mathbf{X},\boldsymbol{\theta},\sigma^2)p(\boldsymbol{\theta}) d\boldsymbol{\theta}} ~,\\\\
    \textrm{posterior} = \frac{\textrm{likelihood} \times \textrm{prior}}{\textrm{evidence}} ~,
    \end{gathered}\]

<p>here \(\mathbf{X}\) is the matrix of individual data points \(\mathbf{x}_i\). The likelihood is multiplied by the prior \(p(\boldsymbol{\theta})\), which can also be designed to express zero prior knowledge about the model also called un-informative prior. We can for example use a uniform prior assigning equal probability for each possible parameter configuration. The product of likelihood and prior has to be normalized to yield a valid probability distribution, which is done by the evidence \(p(\mathbf{Y}\vert\mathbf{X},\sigma^2)\). The evidence or marginal likelihood is the probability for seeing this particular data and is obtained by marginalizing out all possible parameter configurations. Performing these steps yields the posterior \(p(\boldsymbol{\theta}\vert\mathbf{X},\mathbf{Y},\sigma^2)\), which expresses the probability for the parameter given some data.</p>

<p>For a new data point \((\mathbf{x}_*,\mathbf{y}_*)\) the prediction of the model is obtained by considering the predictions made using all possible parameter setting, weighted by their posterior probability:</p>

\[p(\mathbf{y_*}|\mathbf{x_*},\mathbf{Y},\mathbf{X},\sigma^2)=\int p(\mathbf{y_*}|\mathbf{x_*},\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathbf{X},\mathbf{Y},\sigma^2)\mathrm{d}\boldsymbol{\theta} ~.\]

<p>Problematically this integral becomes intractable for even small models that are non-linear.</p>

<p>Before we turn our view to the fully Bayesian approach, we first consider the maximum a posteriori (MAP) estimation of the model parameter. We will again perform polynomial regression, so we will substitute \(\mathbf{X}\) with \(\boldsymbol{\Phi}\). Since we also have a one-dimensional output \(\mathbf{Y}\) becomes \(\mathbf{y}\). The MAP estimate makes use of the proportionality of posterior and the product of likelihood and prior without taking the evidence into account:</p>

\[p(\boldsymbol{\theta}|\mathbf{y},\boldsymbol{\Phi},\beta,\alpha) \propto p(\mathbf{y}|\boldsymbol{\Phi},\boldsymbol{\theta},\beta) p(\boldsymbol{\theta}|\alpha) ~,\]

<p>with a new parameter \(\alpha\) governing the shape of the prior distribution and setting \(\beta=\sigma^{-2}\) for consistency with later chapters. We consider a Gaussian distribution as prior governed by the mentioned precision hyperparameter \(\alpha\)</p>

\[p(\boldsymbol{\theta}|\alpha) = \mathcal{N} (\boldsymbol{\theta}|\mathbf{0},\alpha^{-1}\mathbf{I}) = \left(\frac{\alpha}{2\pi}\right)^{\frac{M+1}{2}} \exp \left\{ - \frac{\alpha}{2} \boldsymbol{\theta}^T\boldsymbol{\theta}\right\} ~,\]

<p>where \(M+1\) is the total number of elements in the vector \(\boldsymbol{\theta}\) for an \(M^{th}\) order polynomial. The optimal parameters can again be found with</p>

\[\hat{\boldsymbol{\theta}} = \displaystyle\textrm{arg max}_{\boldsymbol{\theta}} \; p(\boldsymbol{\theta}|\mathbf{y},\boldsymbol{\Phi},\beta,\alpha) ~.\]

<p>With the probability for the likelihood</p>

\[p(\mathbf{y}|\boldsymbol{\Phi},\boldsymbol{\theta},\beta) = \mathcal{N}(\boldsymbol{\Phi}\boldsymbol{\theta},\beta^{-1}\mathbf{I})\]

<p>the maximum a posteriori estimate becomes</p>

\[\begin{aligned}
    p(\boldsymbol{\theta}|\mathbf{y},\boldsymbol{\Phi},\beta,\alpha) &amp;\propto \exp \left\{  -\frac{\beta}{2} (\mathbf{y} - \boldsymbol{\Phi}\boldsymbol{\theta})^T (\mathbf{y} - \boldsymbol{\Phi}\boldsymbol{\theta}) \right\} \exp \left\{ - \frac{\alpha}{2} \boldsymbol{\theta}^T\boldsymbol{\theta}\right\} \\
    \log p(\boldsymbol{\theta}|\mathbf{y},\boldsymbol{\Phi},\beta,\alpha) &amp;\propto \mathbf{y}^T\mathbf{y} - 2\mathbf{y}^T\boldsymbol{\Phi}\boldsymbol{\theta} + \boldsymbol{\theta}^T\boldsymbol{\Phi}^T\boldsymbol{\Phi}\boldsymbol{\theta} + \frac{\alpha}{\beta} \boldsymbol{\theta}^T\mathbf{I}\boldsymbol{\theta} =\mathcal{L}(\boldsymbol{\theta}) ~.
    \end{aligned}\]

<p>The above equation is referred to as weight decay with factor \(\alpha / \beta\) usually denoted by \(\lambda\), penalizing high values for the parameter \(\boldsymbol{\theta}\) and thereby preventing overfitting. If a Gaussian prior is chosen \(\mathrm{L}_2\) regularization is the consequence, while a Laplacian prior leads to \(\mathrm{L}_1\) regularization. The optimal parameters are obtained by taking the derivative and equating to zero similar to MLE:</p>

\[\begin{gathered}
    \frac{\partial \mathcal{L}(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}} = -2\boldsymbol{\Phi}^T\mathbf{y} + 2\left(\boldsymbol{\Phi}^T\boldsymbol{\Phi} + \frac{\alpha}{\beta}\mathbf{I} \right)\boldsymbol{\theta} ~, \\
    \hat{\boldsymbol{\theta}} = \left(\boldsymbol{\Phi}^T\boldsymbol{\Phi} + \frac{\alpha}{\beta}\mathbf{I}\right)^{-1} \boldsymbol{\Phi}^T\mathbf{y} ~.
    \end{gathered}\]

<p>Intuitively, the above equation for MAP estimation is benefiticial for poorly conditioned problems, where our model exhibits many degrees of freedom but we only have a few data points at our disposal (e.g. in medical studies with few probands but a lot of genes to monitor). Inverting the matrix \(\boldsymbol{\Phi}^T\boldsymbol{\Phi}\) can be ill conditioned, since it might be close to singular because \(\boldsymbol{\Phi}\) has a lot more columns than rows. The problem is solved by adding small values to the diagonal as in above equation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ys_map</span><span class="p">,</span> <span class="n">nlls_map</span><span class="p">,</span> <span class="n">test_mses_map</span><span class="p">,</span> <span class="n">covs_map</span><span class="p">,</span> <span class="n">variances_map</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="n">pol_grads</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="n">max_grad</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">_pol_grads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_grad</span><span class="p">)])</span>

<span class="n">_lambda</span> <span class="o">=</span> <span class="mf">0.6</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">_pol_grads</span><span class="p">:</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_train</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">)]).</span><span class="n">transpose</span><span class="p">()</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">)]).</span><span class="n">transpose</span><span class="p">()</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="n">X</span> <span class="o">+</span> <span class="n">_lambda</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">)))</span> <span class="o">@</span> <span class="n">X</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="n">y_train</span>

    <span class="n">mean</span> <span class="o">=</span> <span class="n">X_test</span> <span class="o">@</span> <span class="n">w</span>
    <span class="n">var</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">).</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">)</span>

    <span class="n">cov</span> <span class="o">=</span> <span class="n">var</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span>

    <span class="n">nll</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">var</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">).</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">)</span>

    <span class="n">train_mse</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span> <span class="o">*</span> <span class="p">((</span><span class="n">y_train</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">test_mse</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">no_test_pts</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">y_test</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">-</span> <span class="n">mean</span><span class="p">[</span><span class="o">-</span><span class="n">no_test_pts</span><span class="p">:])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>

    <span class="n">ys_map</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
    <span class="n">nlls_map</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">nll</span><span class="p">)</span>
    <span class="n">test_mses_map</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_mse</span><span class="p">)</span>
    <span class="n">covs_map</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="n">variances_map</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="c1">#idx_gaussian = np.random.randint(0, len(x_train))
</span><span class="n">pt</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_train</span><span class="p">[</span><span class="n">idx_gaussian</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">idx_gaussian</span><span class="p">]]</span>

<span class="n">ax1</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">'$x_{train}$'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">'$x_{test}$'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pol_grads</span><span class="p">:</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">ys_map</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">_pol_grads</span> <span class="o">==</span> <span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">idx_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">_pol_grads</span> <span class="o">==</span> <span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">ys_map</span><span class="p">[</span><span class="n">idx_y</span><span class="p">]</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">_y</span><span class="p">[</span><span class="n">idx_y</span><span class="p">]</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variances_map</span><span class="p">[</span><span class="n">idx_y</span><span class="p">])</span>
    <span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">ax2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="n">ax1</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'#7F73AF'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">pt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'#7F73AF'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y_true</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$y$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax1</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.6</span><span class="p">,</span><span class="mf">1.6</span><span class="p">])</span>

<span class="n">ax2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$y$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$p(y|x,\boldsymbol{\theta},\sigma^2)$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>

<span class="n">ax2</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">pt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'#7F73AF'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span>
<span class="n">ax2</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">ax3</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">nlls_map</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">nlls_map</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'degree of polynomial'</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$-\log p(\mathbf{y}|\boldsymbol{\Phi},\hat{\boldsymbol{\theta}},\hat{\sigma}^2)$'</span><span class="p">,</span>
                      <span class="n">color</span><span class="o">=</span><span class="s">'#748ABA'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">ax3</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">nlls</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">ax31</span> <span class="o">=</span> <span class="n">ax3</span><span class="p">.</span><span class="n">twinx</span><span class="p">()</span>
<span class="n">ax31</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_mses_map</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">test_mses_map</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">)</span>
<span class="n">ax31</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\textrm{MSE}(y_{test}, \hat{y}_{test})$'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">ax31</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ax3</span><span class="p">.</span><span class="n">get_ylim</span><span class="p">())</span>
<span class="n">ax31</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/linear_regression_files/linear_regression_9_0.png" alt="png" /></p>

<h2 id="bayesian-linear-regression">Bayesian Linear Regression</h2>

<p>Solving the marginal likelihood in Bayes Theorem is often intractable, but for simpler models such as linear regression it is possible, if the chosen probability distributions fulfill certain criteria. The most important one is the conjugacy of likelihood and prior, which states that both are from the same family of distributions. Gaussian distributions fulfill this criteria since a product of two Gaussians is again a Gaussian.<br />
In order to compute the posterior mean and variance for the linear regression of our example, we first need to look at the shape of the posterior, excluding all terms that are not a function of the model parameters:</p>

\[\begin{aligned}
    p(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y},\sigma^2) &amp;= \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})\\
     &amp;\propto \exp \left\{ -\frac{1}{2}\left(\boldsymbol{\theta} - \boldsymbol{\mu}\right)^T \boldsymbol{\Sigma}^{-1} \left(\boldsymbol{\theta} - \boldsymbol{\mu}\right)  \right\} \\
    &amp;\propto \exp \left\{ -\frac{1}{2} \left(\boldsymbol{\theta}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\theta} - 2\boldsymbol{\theta}^T \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu} \right) \right\} ~.
    \end{aligned}\]

<p>As Bayes Theorem states the posterior can also be expressed as a product of likelihood and prior up to a proportionality constant, the marginal likelihood:</p>

\[\begin{gathered}
    p(\mathbf{y}|\mathbf{X},\boldsymbol{\theta},\sigma^2)=\mathcal{N}(\boldsymbol{\Phi}\boldsymbol{\theta},\sigma^2\mathbf{I})~,\;\;\;\;p(\boldsymbol{\theta})=\mathcal{N}(\boldsymbol{\mu}_0,\mathbf{S}_0)~, \\
    \begin{aligned}
    p(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y},\sigma^2) &amp;\propto \exp \left\{  -\frac{1}{2\sigma^2} \left(\mathbf{y} - \boldsymbol{\Phi}\boldsymbol{\theta}\right)^T\left(\mathbf{y} - \boldsymbol{\Phi}\boldsymbol{\theta}\right) \right\} \exp \left\{ -\frac{1}{2}\left(\boldsymbol{\theta}-\boldsymbol{\mu}_0\right)\mathbf{S}_{0}^{-1} \left(\boldsymbol{\theta} -\boldsymbol{\mu}_0\right)\right\} \\
    &amp;\propto \exp \left\{ -\frac{1}{2} \left[\boldsymbol{\theta}^T\left(\frac{1}{\sigma^2}\boldsymbol{\Phi}^T\boldsymbol{\Phi} + \mathbf{S}_0^{-1}\right)\boldsymbol{\theta} + \boldsymbol{\theta}^T\left( \frac{1}{\sigma^2} \boldsymbol{\Phi}^T\mathbf{y} + \mathbf{S}_0^{-1}\boldsymbol{\mu}_0\right)\right]\right\} ~.
    \end{aligned}
    \end{gathered}\]

<p>By matching the two expression in the above equations we can find the expressions for posterior mean and variance:</p>

\[\boldsymbol{\Sigma}^{-1} = \frac{1}{\sigma^2}\boldsymbol{\Phi}^T\boldsymbol{\Phi}+\mathbf{S}_0^{-1}~,\;\;\;\; \boldsymbol{\mu} = \boldsymbol{\Sigma}\left(\frac{1}{\sigma^2}\boldsymbol{\Phi}^T\mathbf{y} + \mathbf{S}_0^{-1}\boldsymbol{\mu}_0\right) ~.\]

<p>We notice that, if the prior is infinitely broad, meaning \(\mathbf{S}_0^{-1} \rightarrow \mathbf{0}\), the maximum likelihood estimation for the mean is recovered. To express no previous knowledge in the regression example, we will consider a fairly wide prior, an isotropic Gaussian with zero mean and variance governed by the precision parameter \(\alpha\), which is the inverse of the variance</p>

\[P(\boldsymbol{\theta}|\alpha) = \mathcal{N}(\mathbf{0},\alpha^{-1}\mathbf{I}) ~.\]

<p>By additionally setting \(\beta = \sigma^{-2}\) the posterior parameters become</p>

\[\boldsymbol{\Sigma}^{-1}=\beta\boldsymbol{\Phi}^T\boldsymbol{\Phi}+\alpha\mathbf{I}~,\;\;\;\; \boldsymbol{\mu}=\beta\boldsymbol{\Sigma}\boldsymbol{\Phi}^T\mathbf{y} ~.\]

<p>The main use of the posterior is to compute the predictive distribution for a new data point \((\mathbf{x}_*,\mathbf{y}_*)\) of the model as shown in Eq. \ref{eq:pred_post}. Since both distributions of that integral are Gaussians,</p>

\[p(\boldsymbol{\theta}|\boldsymbol{\Phi},\mathbf{y},\beta,\alpha) = \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma}) ~,\;\;\;\; p(\mathbf{y}_*|\boldsymbol{\Phi}_*,\boldsymbol{\theta},\beta)=\mathcal{N}(\boldsymbol{\theta}^T\boldsymbol{\Phi}_*,\beta^{-1}) ~,\]

<p>the integral can be solved analytically and the predictive distribution is Gaussian as well:</p>

\[p(\mathbf{y}_*|\boldsymbol{\Phi}_*,\boldsymbol{\Phi},\mathbf{y},\beta,\alpha) = \mathcal{N}(\boldsymbol{\mu}^T\boldsymbol{\Phi}_*,\beta^{-1}+\boldsymbol{\Phi}_*^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\Phi}_*) ~.\]

<p>As can be seen the predictive variance is the sum of two components, the first one being the noise in the data \(\beta^{-1}\), which is constant and gets more accurate with more training data. The second component depends on the covariance of the parameters, hence an uncertain model model will make uncertain predictions. Moreover, with seeing more data the variance converges to \(\beta^{-1}\), because the covariance of the parameters decreases.</p>

<p>So far we have assumed the parameters \(\alpha\) and \(\beta\) are known, however, in a fully Bayesian treatment we also have to impose prior distributions over these parameters. Unfortunately, the solution is not analytically tractable, so that an iterative approach known as Empirical Bayes has to be employed. The analytical intractability comes from the need for evaluating the evidence function</p>

\[p(\mathbf{y}|\boldsymbol{\Phi},\beta,\alpha) = \int p(\mathbf{y}|\boldsymbol{\Phi},\boldsymbol{\theta},\beta)p(\boldsymbol{\theta}|\alpha)d\boldsymbol{\theta} ~.\]

<p>Since even for simple models their exists no analytical solution, an iterative approach must be employed, which includes maximizing the marginal likelihood or evidence w.r.t. \(\alpha\) and \(\beta\).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">x</span><span class="p">[:,</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]))</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

<span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">W1</span><span class="p">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">W1</span>
<span class="n">pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">W2</span>

<span class="n">no_samples</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="s">'all'</span><span class="p">]</span>
<span class="n">valid_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))]</span>

<span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">25.</span> <span class="c1"># 0.3, 1.
</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">cov_inv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">alpha</span>

<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>

<span class="n">x_ts</span><span class="p">,</span> <span class="n">y_ts</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([]),</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([])</span>

<span class="n">pts_total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">_no_samples</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">no_samples</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">_no_samples</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_no_samples</span> <span class="o">!=</span> <span class="s">'all'</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">valid_idx</span><span class="p">,</span> <span class="n">_no_samples</span> <span class="o">-</span> <span class="n">pts_total</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">pts_total</span> <span class="o">=</span> <span class="n">_no_samples</span>
            <span class="k">for</span> <span class="n">_idx</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">idx</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="k">del</span> <span class="n">valid_idx</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">valid_idx</span><span class="o">==</span><span class="n">_idx</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span>
            <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">y_noisy</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="n">x_ts</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">x_ts</span><span class="p">,</span> <span class="n">x_t</span><span class="p">))</span>
            <span class="n">y_ts</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">y_ts</span><span class="p">,</span> <span class="n">y_t</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span>
            <span class="n">x_ts</span><span class="p">,</span> <span class="n">y_ts</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">y_noisy</span>

        <span class="n">X_t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_t</span><span class="o">**</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pol_grad</span><span class="p">)]).</span><span class="n">transpose</span><span class="p">()</span>

        <span class="n">_cov_inv</span> <span class="o">=</span> <span class="n">cov_inv</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">X_t</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="n">X_t</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">_cov_inv</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">cov</span> <span class="o">@</span> <span class="p">(</span><span class="n">cov_inv</span> <span class="o">@</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">X_t</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="o">@</span> <span class="n">y_t</span><span class="p">)</span>

        <span class="n">cov_inv</span> <span class="o">=</span> <span class="n">_cov_inv</span>

    <span class="n">y_pred_mean</span> <span class="o">=</span> <span class="n">_x</span> <span class="o">@</span> <span class="n">mu</span>
    <span class="n">y_pred_var</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span> <span class="o">/</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">__x</span> <span class="o">@</span> <span class="n">cov</span> <span class="o">@</span> <span class="n">__x</span><span class="p">.</span><span class="n">transpose</span><span class="p">()</span> <span class="k">for</span> <span class="n">__x</span> <span class="ow">in</span> <span class="n">_x</span><span class="p">])</span>

    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax_weight_posterior</span><span class="p">,</span> <span class="n">ax_posterior</span><span class="p">,</span> <span class="n">ax_posterior_2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

    <span class="n">ax_posterior</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'predictive mean'</span><span class="p">)</span>
    <span class="n">ax_posterior</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_pred_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_pred_var</span> <span class="o">**</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">y_pred_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">y_pred_var</span> <span class="o">**</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span>
                              <span class="n">color</span><span class="o">=</span><span class="s">'#6A83B5'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s">'$2\sigma$'</span><span class="p">)</span>
    <span class="n">ax_posterior</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span><span class="c1">#'#D1895C')
</span>    <span class="n">ax_posterior</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_ts</span><span class="p">,</span> <span class="n">y_ts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
    <span class="n">ax_posterior</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">])</span>
    <span class="n">ax_posterior</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)])</span>
    <span class="n">ax_posterior</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax_posterior</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$y$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax_posterior</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
    <span class="n">ax_posterior</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">label</span> <span class="o">=</span> <span class="s">'predictions'</span> <span class="k">if</span> <span class="n">_</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="s">''</span>
        <span class="n">w_sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span>
        <span class="n">ax_posterior_2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">_x</span> <span class="o">@</span> <span class="n">w_sample</span><span class="p">[:,</span><span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]).</span><span class="n">flatten</span><span class="p">())</span><span class="c1">#, color='#6A83B5', label=label)
</span>    <span class="n">ax_posterior_2</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span>
    <span class="n">ax_posterior_2</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_ts</span><span class="p">,</span> <span class="n">y_ts</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
    <span class="n">ax_posterior_2</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)])</span>
    <span class="n">ax_posterior_2</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ax_posterior</span><span class="p">.</span><span class="n">get_ylim</span><span class="p">())</span>
    <span class="n">ax_posterior_2</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax_posterior_2</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$y$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax_posterior_2</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
    <span class="n">ax_posterior_2</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>

    <span class="n">ax_weight_posterior</span><span class="p">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">stats</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="o">/</span><span class="mf">0.1</span><span class="p">).</span><span class="n">pdf</span><span class="p">(</span><span class="n">pos</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">ax_weight_posterior</span><span class="p">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">color</span><span class="o">=</span><span class="s">'white'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'true'</span><span class="p">)</span>
    <span class="n">ax_weight_posterior</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)])</span>
    <span class="n">ax_weight_posterior</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)])</span>
    <span class="n">ax_weight_posterior</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\theta_0$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax_weight_posterior</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$\theta_1$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax_weight_posterior</span><span class="p">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax_weight_posterior</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/linear_regression_files/linear_regression_11_0.png" alt="png" /></p>

<p><img src="/assets/imgs/linear_regression_files/linear_regression_11_1.png" alt="png" /></p>

<p><img src="/assets/imgs/linear_regression_files/linear_regression_11_2.png" alt="png" /></p>

<p><img src="/assets/imgs/linear_regression_files/linear_regression_11_3.png" alt="png" /></p>

<p><img src="/assets/imgs/linear_regression_files/linear_regression_11_4.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>]]></content><author><name>Malte Tölle</name><email>malte.toelle@med.uni-heidelberg.de</email></author><category term="linear" /><category term="regression" /><summary type="html"><![CDATA[For an introduction to deep learning it is generally best to start with the simplest example of linear regression. In linear regression we want to obtain a model that best describes the correlation between input data points \(\mathbf{x}\) and output data points \(\mathbf{y}\) with some model parameters. For the sake of visualization our output dimensionality is one and our input dimensionality is \(D\):]]></summary></entry></feed>