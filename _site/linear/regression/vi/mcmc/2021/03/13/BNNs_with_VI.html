
<!DOCTYPE html>
<html lang="en">
head>
 <meta charset="utf-8">
 <meta name="viewport" content="width=device-width,initial-scale=1">
 <title> Malte Tölle </title>
 <link href="https://fonts.googleapis.com/css?family=Roboto+Mono|Roboto+Slab:300|Roboto:500" rel="stylesheet">
 <link rel="stylesheet" href="/assets/css/styles.css">
 <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
 <script src="/assets/js/jquery-cookie/src/jquery.cookie.js" type="text/javascript"></script>
 <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
    "HTML-CSS": {
     styles: {

       ".MJXc-display": {
         // "background-color": "#FFFF88",
         // "color":   "#CC0000",
         // "border":  "1px solid #CC0000",
         // "padding": "1px 3px",
         // "font-family": "serif",
         // "font-style": "normal",
         "font-size":  "10%"
       },

       ".MathJax_Preview": {color: "#888888"},

     }
    },
     extensions: [
       "MathMenu.js",
       "MathZoom.js",
       "AssistiveMML.js",
       "a11y/accessibility-menu.js"
     ],
     jax: ["input/TeX", "output/CommonHTML"],
     TeX: {
       extensions: [
         "AMSmath.js",
         "AMSsymbols.js",
         "noErrors.js",
         "noUndefined.js",
       ]
     }

   });
 </script>
 <script type="text/javascript" async
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
 </script>
</head>

  <!--<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Bayesian Neural Networks with Variational Inference | Malte Tölle</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Bayesian Neural Networks with Variational Inference" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The general concept of variational inference (VI) and one application to linear regression can be found here. We will quickly review the basic concepts needed to understand the following." />
<meta property="og:description" content="The general concept of variational inference (VI) and one application to linear regression can be found here. We will quickly review the basic concepts needed to understand the following." />
<link rel="canonical" href="http://localhost:4000/linear/regression/vi/mcmc/2021/03/13/BNNs_with_VI.html" />
<meta property="og:url" content="http://localhost:4000/linear/regression/vi/mcmc/2021/03/13/BNNs_with_VI.html" />
<meta property="og:site_name" content="Malte Tölle" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-03-13T15:58:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Bayesian Neural Networks with Variational Inference" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/linear/regression/vi/mcmc/2021/03/13/BNNs_with_VI.html"},"url":"http://localhost:4000/linear/regression/vi/mcmc/2021/03/13/BNNs_with_VI.html","headline":"Bayesian Neural Networks with Variational Inference","dateModified":"2021-03-13T15:58:00+01:00","datePublished":"2021-03-13T15:58:00+01:00","description":"The general concept of variational inference (VI) and one application to linear regression can be found here. We will quickly review the basic concepts needed to understand the following.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Malte Tölle" /></head>
-->

  <body><div class="header">
 <div class="nav-click header-title-div">
 <a href="#intro" class="header-title"> Malte Tölle </a> </div>
 <div class="nav-bar nav-click">
   <a href="#pubs">Pubs</a>
   <a href="#posts">Blog</a>
   <a href="/contact">Contact</a>
 </div>
</div>
<main class="page-content" aria-label="Content">
     <div class="cookie-blocker"></div>
     <div class="wrapper">
      <div class="content">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bayesian Neural Networks with Variational Inference</h1>
    <p class="post-meta"><time class="dt-published" datetime="2021-03-13T15:58:00+01:00" itemprop="datePublished">
        Mar 13, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>The general concept of variational inference (VI) and one application to linear regression can be found <a href="">here</a>. We will quickly review the basic concepts needed to understand the following.</p>

<p>In contrast to maximum likelihood, which assumes one true parameter set \(\hat{\boldsymbol{\theta}}\) that best explains our data, the Bayesian approach imposes a prior distribution onto the parameters and thereby treating them as random variables. A comparison for the simplest case of linear regression can be found <a href="https://maltetoelle.github.io/linear/regression/2020/10/27/try.html">here</a>. For a data set \(\mathcal{D}=(\mathbf{x}_i,\mathbf{y}_i)\) Bayes theorem is given by</p>

\[p(\boldsymbol{\theta}|\mathcal{D})=\frac{p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathcal{D})}=\frac{p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})}{\int p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})\,d\boldsymbol{\theta}}~,\]

<p>where the posterior of our model parameters \(p(\boldsymbol{\theta}\vert\mathcal{D})\) is obtained by multiplying the likelihood \(p(\mathcal{D}\vert\boldsymbol{\theta})\), the probability for seeing this data with our model parameters, with the prior \(p(\boldsymbol{\theta})\), our assumption for the parameter distribution of the parameters before seeing any data.</p>

<p>This can also be an uniform (non-informative) prior, assigning the same probability to all parameter distributions. To obtain a valid probability distribution the product of the two must be normalized to integrate to one, which is done by dividing by the evidence \(p(\mathcal{D})\) that is obtained by marginalizing out all possible parameter distributions. Even for simple models that are non-linear this calculation becomes intractable. So we must help ourselves with approximation frameworks such as variational inference or Markov Chain Monte Carlo (MCMC) sampling, which was compared to linear regression with variational inference in the same <a href="">post</a>.</p>

<p>The predictive distribution for a new data point \((\mathbf{x}_*,\mathbf{y}_*)\) is obtained in the Bayesian framework by marginalizing out the posterior parameter distribution</p>

\[p(\mathbf{y}_*|\mathbf{x}_*,\mathcal{D})=\int p(\mathbf{y}_*|\mathbf{x}_*,\mathcal{D},\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathcal{D})\,d\boldsymbol{\theta}~.\]

<p>In VI we use a simpler distribution \(q(\boldsymbol{\theta})\) to approximate our intractable posterior. The optimization objective is then given by the minimum of the KL divergence between approsimate and true posterior</p>

\[F(q):=\mathrm{KL}(q(\boldsymbol{\theta})||p(\boldsymbol{\theta}|\mathcal{D}))=\int q(\boldsymbol{\theta})\log \frac{q(\boldsymbol{\theta})}{p(\boldsymbol{\theta}|\mathcal{D})}\,\mathrm{d}\boldsymbol{\theta} \longrightarrow \underset{q(\boldsymbol{\theta}) \in \mathcal{Q}}{\min} ~.\]

<p>Although it is not a true distance measure because of its asymmetry, it can be seen as one, as it has its minimum of zero, if and only if both distributions are equal. For all other distributions it is always greater than zero. Befor we dive deeper into the derivations for applying VI to neural networks, we will quickly revisit the important properties of the KL divergence, as they come in handy later.</p>

<h2 id="kl-divergence">KL Divergence</h2>

<p>The KL divergence is defined as (<a href="https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729694">Kullback and Leibler 1951</a>)</p>

\[\begin{align}
\mathrm{KL}(q(x)||p(x)) &amp;= \textrm{H}(q(x),p(x)) - \textrm{H}(q(x)) \\
&amp;= - \int q(x) \log p(x)\,dx - \left( - \int q(x)\log q(x)\,dx \right) \\
&amp;= \int q(x)\log\frac{q(x)}{p(x)}\,dx~,
\end{align}\]

<p>where \(\textrm{H}(q(x),p(x))\) denotes the cross-entropy between \(q\) and \(p\) and \(\textrm{H}(q(x))\) is the entropy of \(q\). Formally, the KL divergence measures, how much information is lost, when p is approximated by q or vice versa. For two Gaussian distributions the KL divergence can be computed analytically</p>

\[\begin{aligned}
\mathrm{KL}(q(x)||p(x)) &amp;= \int q(x)\log q(x)\,dx - \int q(x)\log p(x)\,dx\\
&amp;= \frac{1}{2}\log\left(2\pi\sigma_p^2\right) + \frac{\sigma_q^2 + \left( \mu_q - \mu_p \right)^2}{2\sigma_p^2} - \frac{1}{2}\left( 1 + \log\left(2\pi\sigma_q^2\right) \right)\\
&amp;= \log\frac{\sigma_p}{\sigma_q} + \frac{\sigma_q^2 + \left( \mu_q - \mu_p \right)^2}{2\sigma_p^2} - \frac{1}{2} ~.
\end{aligned}\]

<p>For more complicated distributions, where the KL divergence is not analytically tractable, but the expectation can be approximated using Monte Carlo (MC) samples:</p>

\[\mathrm{KL}(q(x)||p(x)) = \mathbb{E}_{x\sim q}\left[ \log\frac{q(x)}{p(x)} \right] \approx \sum_{i=0}^{N}\left( \log q(x_i) - \log p(x_i) \right)~.\]

<p>An example for a more complicated distribution is the mixture of Gaussians:</p>

\[p(x)=\sum_i \pi_i \mathcal{N}(x|\mu_i,\sigma_i)~.\]

<p>The important properties of the KL divergence are:</p>

\[\begin{aligned}
    \textrm{non-negativity}&amp;: \quad \mathrm{KL}(q(x)||p(x)) \geq 0 ~, \;\; \forall x ~,\\
    \textrm{equality}&amp;: \quad \mathrm{KL}(q(x)||p(x)) = 0 \quad \textrm{if and only if} \quad q(x) = p(x) ~,\\
   \textrm{asymmetry}&amp;: \quad \mathrm{KL}(q(x)||p(x)) \neq \mathrm{KL}(p(x)||q(x)) ~.
\end{aligned}\]

<p>The KL divergence becomes zero, if and only if both distributions are equal. For all other distributions it is always greater than zero. As already said it is not a true distance metric becaus of its asymmetry. We distinguish between the reverse (exclusive) \(\mathrm{KL}(q(x)\vert\vert p(x))\) and the forward (inclusive) \(\mathrm{KL}(p(x)\vert\vert q(x))\) case. The following illustrates both cases.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.distributions.distribution</span> <span class="kn">import</span> <span class="n">Distribution</span>
<span class="kn">from</span> <span class="nn">torch.distributions.normal</span> <span class="kn">import</span> <span class="n">Normal</span>

<span class="k">def</span> <span class="nf">mc_kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">kl</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">rsample</span><span class="p">()</span>
        <span class="n">kl</span> <span class="o">+=</span> <span class="n">p</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="n">q</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">kl</span> <span class="o">/</span> <span class="n">n_samples</span>

<span class="k">class</span> <span class="nc">MixtureNormal</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">pi</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MixtureNormal</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">loc</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi</span><span class="p">))</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi</span><span class="p">))</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">loc</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">dists</span> <span class="o">=</span> <span class="p">[</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span> <span class="k">for</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">loc</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">rsample</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">pi</span><span class="p">,</span> <span class="n">dist</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dists</span><span class="p">):</span>
            <span class="n">rsample</span> <span class="o">+=</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">dist</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">dist</span><span class="p">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">rsample</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">pdf</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">pi</span><span class="p">,</span> <span class="n">dist</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dists</span><span class="p">):</span>
            <span class="n">pdf</span> <span class="o">+=</span> <span class="n">pi</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">dist</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="p">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s">"ignore"</span><span class="p">,</span> <span class="nb">UserWarning</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="p">.</span><span class="nb">set</span><span class="p">()</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">matplotlib</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'text.usetex'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">matplotlib</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'text.latex.preamble'</span><span class="p">]</span> <span class="o">=</span> <span class="s">r'\usepackage{bm}'</span>

<span class="k">def</span> <span class="nf">optimize_kl</span><span class="p">(</span><span class="n">p</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span> <span class="n">intial_mu_q</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">intial_sigma_q</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'forward'</span><span class="p">,</span>
                <span class="n">num_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>

    <span class="c1"># intial values for mu_q and sigma_q
</span>    <span class="n">mu_q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">intial_mu_q</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">sigma_q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">intial_sigma_q</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">mu_q</span><span class="p">,</span> <span class="n">sigma_q</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">)):</span>
        <span class="n">optim</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu_q</span><span class="p">,</span> <span class="n">sigma_q</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">kl_type</span> <span class="o">==</span> <span class="s">'forward'</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">mc_kl_divergence</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">mc_kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mu_q</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">sigma_q</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">1.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">MixtureNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="n">mu1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="p">[</span><span class="n">sigma1</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">],</span> <span class="n">pi</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">kl_type</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s">'reverse'</span><span class="p">,</span> <span class="s">'forward'</span><span class="p">]):</span>
    <span class="n">mu_q</span><span class="p">,</span> <span class="n">sigma_q</span> <span class="o">=</span> <span class="n">optimize_kl</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">intial_mu_q</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">intial_sigma_q</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu_q</span><span class="p">,</span> <span class="n">sigma_q</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">exp</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s">r'$p(x)$'</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">q</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">exp</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s">r'$q(x)$'</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">r'$x$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">r'$p(x)$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="n">kl_type</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">22</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 1000/1000 [00:37&lt;00:00, 26.65it/s]
100%|██████████| 1000/1000 [00:30&lt;00:00, 32.34it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_3_1.png" alt="png" /></p>

<p>To better understand the two behaviours we brake down the KL divergence piece by piece. As can be seen in the reverse case the KL divergence approximates the mode of the \(p\) and thereby leaving a lot of mass uncovered. As we would like to minimize the KL divergence we must minimize the \(\log\frac{q}{p}\) term, which is small in areas, where \(p\) is large. Thereby the objective converges to a mode of \(p\). Consequently, in the forward case on the other hand the objective gets small in areas, where \(p\) is small.</p>

<h2 id="log-evidence-lower-bound">(Log) Evidence Lower Bound</h2>

<p>One can rewrite the log evidence such that we are left with the KL divergence from above and another term, named the (log) evidence lower bound or short ELBO</p>

\[\begin{aligned}
\log p(\mathcal{D}) &amp;= \int q(\boldsymbol{\theta})\log\frac{p(\mathcal{D},\boldsymbol{\theta})}{q(\boldsymbol{\theta})}\,d\boldsymbol{\theta} + \int q(\boldsymbol{\theta})\frac{q(\boldsymbol{\theta})}{p(\boldsymbol{\theta}|\mathcal{D})}\,d\boldsymbol{\theta} \\
&amp;= \textrm{ELBO}(q(\boldsymbol{\theta})) + \mathrm{KL}(q(\boldsymbol{\theta})||p(\boldsymbol{\theta}|\mathcal{D})) \\
&amp;\geq \textrm{ELBO}(q(\boldsymbol{\theta})) ~.
\end{aligned}\]

<p>As the KL divergence is always greater than zero, maximizing the first term, the ELBO, w.r.t. \(q\) is essentially equal to minimizing the second term, the KL divergence.</p>

<p>When applying the ELBO as optimization criterion to neural networks it must further be simplified into a data-dependent likelihood term and a regularizer measuring the “distance” between prior and posterior.</p>

\[\begin{aligned}
\textrm{ELBO}(q(\boldsymbol{\theta})) &amp;= \int q(\boldsymbol{\theta}) \log \frac{p(\mathcal{D}|\boldsymbol{\theta})p(\boldsymbol{\theta})}{q(\boldsymbol{\theta})}\,d\boldsymbol{\theta}  \\
&amp;= \int q(\boldsymbol{\theta}) \log p(\mathcal{D}|\boldsymbol{\theta})\,d\boldsymbol{\theta} + \int q(\boldsymbol{\theta})\log\frac{p(\boldsymbol{\theta})}{q(\boldsymbol{\theta})}\,d\boldsymbol{\theta} \\
&amp;= \underbrace{\mathbb{E}_{\boldsymbol{\theta}\sim q} \log p(\mathcal{D}|\boldsymbol{\theta})}_{\textrm{likelihood term}} - \underbrace{\mathrm{KL}(q(\boldsymbol{\theta})||p(\boldsymbol{\theta}))}_{\textrm{regularizer}}
\end{aligned}\]

<p>In each optimization step we sample weights \(\boldsymbol{\theta}\) from our approximate posterior \(q(\boldsymbol{\theta})\). The expectation of the likelihood term then measures, how well on average our model fits the data. The KL divergence measures how well our approximate posterior matches our prior. Since it must be minimized in order to minimize the ELBO, the model tries to keep the posterior as close as possible to the prior. For computing the above KL divergence we can now either use the forward or reverse version.</p>

<p>Now, the question remains to which family of distributions \(\mathcal{Q}\) to restrict \(q\) to allow for tractable soluitons for approximating the true posterior. We can either use a parametric distribution \(q_{\boldsymbol{\omega}}(\boldsymbol{\theta})\) governed by a set of parameters \(\boldsymbol{\omega}\). Hence, the ELBO becomes a function of \(\boldsymbol{\omega}\), and we can exploit standard non-linear optimization techniques to determine the optimal values for the parameters. Another possibility is to use factorized distributions. We will revisit both concepts in the domain of neural networks and apply them to the case of non-linear regression.</p>

<h2 id="mean-field-assumption">Mean Field Assumption</h2>

<p>As was the case in the linear regression example we start by using a factorized distribution as approximation, known as mean field approximation</p>

\[q(\boldsymbol{\theta})=\prod_i q_i(\boldsymbol{\theta}_i)~,\]

<p>which discards covariances in the parameters because of the factorization, but leads to faster computation time as this decreases the number of parameters to optimize as well. To model each individual \(q_i(\boldsymbol{\theta}_i)\) we will use a Gaussian distribution \(\mathcal{N}(\boldsymbol{\theta}_i\vert\boldsymbol{\mu}_i,\boldsymbol{\sigma}_i)\) as this is justified under the Bayesian central limit theorem. As can be seen in the comparison of MCMC sampling and VI for linear regression this assumption usually leads to an underestimation of the variance in the parameters.</p>

<p>When we want to apply the mean field approximation to a neural network a problem arises, when we want to apply the backpropagation algorithm to a probability distribution. The general update formula of gradient descent, which lies at the heart of backpropagation, is given by</p>

\[\theta_{ij}^* = \theta_{ij} - \eta \frac{\partial\mathcal{L}(\boldsymbol{\theta})}{\partial\theta_{ij}}~,\]

<p>where the new value \(\theta_{ij}^*\) for each weight after every iteration is obtained by subtracting the partial derivate of some loss function \(\mathcal{L}(\boldsymbol{\theta})\) (e.g. mean squared error, cross-entropy) w.r.t. that particular weight \(\theta_{ij}\) weighted by a learning rate \(\eta\) from \(\theta_{ij}\). The above formula can be applied to point estimates of the parameters only, each weight must have one explicitly defined value. Following from that, it is not applicable to probability distributions in their standard form.</p>

<h3 id="local-reparametrization-trick">(Local) Reparametrization Trick</h3>

<p>Remedy comes in the form of the reparametrization trick, which separates the deterministic and stochastic components of the weights (<a href="https://arxiv.org/abs/1312.6114">Kingma and Welling 2013</a>, <a href="https://arxiv.org/abs/1505.05424">Blundell et al. 2015</a>). Instead of sampling weights directly from \(q(\boldsymbol{\theta})\) the mean and the variance of the Gaussians modelling the weights are treated as parameters and another random variable \(\boldsymbol{\epsilon}\) is introduced:</p>

\[\begin{gathered}
\theta_{ij} \sim \mathcal{N}(\mu_{ij},\sigma_{ij}) ~,\\
\theta_{ij} = \mu_{ij} + \sigma_{ij}\epsilon_{ij} \quad \textrm{with} \quad \epsilon_{ij} \sim \mathcal{N}(0,1) ~.
\end{gathered}\]

<p>In each forward pass the weights are sampled according to the formula above and then, subsequently, the partial derivate w.r.t. to mean and variance is computed. This essentially means the number of parameters is doubled. To ensure an always positive variance, the reparametrization is usually extended with the Softplus function:</p>

\[\theta_{ij}=\mu_{ij} + \log\left( 1+ \exp\left(\sigma_{ij}^2\right) \right)\epsilon_{ij}~.\]

<p>The following images explain the induced differences of using reparametrization visually. On the left the initial situation is presented, where each weight \(\theta\) is modelled with a distribution \(q\). On the right the reparametrization trick is shown, in which in each forward pass a weight is sampled.</p>

<div>
  <img src="/assets/imgs/BNNs_with_VI_files/usual-1.png" width="200" />
  <img src="/assets/imgs/BNNs_with_VI_files/rt-1.png" width="200" />
</div>

<p>The reparametrization trick described above still exhibits limitations concerning the variance. If we were to sample one weight for each mini-batch, the resulting outputs would show high covariances. We could circumvent this problem by sampling a separate weight for each sample in the mini-batch, but this is computational expensive. <a href="https://arxiv.org/abs/1506.02557">Kingma et al. (2015)</a> first discovered that for a factorized Gaussian posterior on the weights, the posterior on the activations is also a factorized Gaussian. Thus, instead of sampling the weights directly we can also sample from the pre-activation neuron. <a href="https://arxiv.org/abs/1506.02557">Kingma et al. (2015)</a> report much lower variance and computational time for their gradient estimator termed local reparametrization trick. We will conduct a comparison later, when we have implemented both. More formally, their reparametrization trick is mathematically given by</p>

\[\begin{gathered}
q_{\omega}(\theta_{ij})=\mathcal{N}(\mu_{ij},\sigma_{ij}^{2}) \;\; \forall \;\; \theta_{ij} \in \boldsymbol{\theta} \quad \Longrightarrow \quad q_{\omega}(b_{mj}|\mathbf{A})=\mathcal{N}(\gamma_{mj},\delta_{mj}) ~,\\
\gamma_{mj}=\sum_{i} a_{mi}\mu_{ij} ~,\\
\delta_{mj}=\sum_{i} a_{mi}^{2}\sigma_{ij}^{2} ~,\\
b_{mj}=\gamma_{mj}+\sqrt{\delta_{mj}}\epsilon_{mj}\quad\textrm{with}\quad \epsilon_{mj} \sim \mathcal{N}(0,1) ~,
\end{gathered}\]

<p>where \(\boldsymbol{\epsilon}\) is a matrix of the same size as \(\mathbf{B}\). The difference between the two reparametrization tricks is visualized in the following.</p>

<div>
  <img src="/assets/imgs/BNNs_with_VI_files/rt-1.png" width="200" />
  <img src="/assets/imgs/BNNs_with_VI_files/lrt-1.png" width="200" />
</div>

<h3 id="implementation">Implementation</h3>

<h4 id="general-vi-module">General VI Module</h4>

<p>To later be able to extend the concept to the local reparametrization trick, we start by implementing a general VI module, which lies at the heart of all further layers.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torch.distributions.kl</span> <span class="kn">import</span> <span class="n">kl_divergence</span>

<span class="k">class</span> <span class="nc">VIModule</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">layer_fct</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                 <span class="n">weight_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
                 <span class="n">bias_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'reverse'</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">VIModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># function for forward pass e.g. F.linear, F.conv2d
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">layer_fct</span> <span class="o">=</span> <span class="n">layer_fct</span>

        <span class="c1"># fall back to default vals
</span>        <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">prior</span> <span class="o">=</span> <span class="p">{</span><span class="s">'mu'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">}</span>

        <span class="k">if</span> <span class="n">posteriors</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">posteriors</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">'mu'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
                <span class="s">'rho'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
            <span class="p">}</span>

        <span class="c1"># if prior is ScaleMixture we must use MC integration for KL div.
</span>        <span class="c1"># otherwise we can compute KL div. analitically
</span>        <span class="k">if</span> <span class="s">'pi'</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">prior</span><span class="p">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_kl_divergence</span> <span class="o">=</span> <span class="n">mc_kl_divergence</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">MixtureNormal</span><span class="p">(</span><span class="n">prior</span><span class="p">[</span><span class="s">'mu'</span><span class="p">],</span> <span class="n">prior</span><span class="p">[</span><span class="s">'sigma'</span><span class="p">],</span> <span class="n">prior</span><span class="p">[</span><span class="s">'pi'</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_kl_divergence</span> <span class="o">=</span> <span class="n">kl_divergence</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">prior</span><span class="p">[</span><span class="s">'mu'</span><span class="p">],</span> <span class="n">prior</span><span class="p">[</span><span class="s">'sigma'</span><span class="p">])</span>

        <span class="c1"># either 'forward' or 'reverse'
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">kl_type</span> <span class="o">=</span> <span class="n">kl_type</span>

        <span class="c1"># save parameters for resetting
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">posterior_mu_initial</span> <span class="o">=</span> <span class="n">posteriors</span><span class="p">[</span><span class="s">'mu'</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">posterior_rho_initial</span> <span class="o">=</span> <span class="n">posteriors</span><span class="p">[</span><span class="s">'rho'</span><span class="p">]</span>

        <span class="c1"># initialize weights and biases
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">W_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">weight_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_rho</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">weight_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bias_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">bias_size</span><span class="p">))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias_rho</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">bias_size</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'bias_mu'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s">'bias_rho'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

        <span class="c1"># reset
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_mu</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_mu_initial</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_rho</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_rho_initial</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_mu_initial</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias_rho</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">posterior_rho_initial</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">kl</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># compute KL div. by instantiating the weights as Normal distribution
</span>        <span class="n">_kl</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W_mu</span><span class="p">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W_rho</span><span class="p">).</span><span class="n">cpu</span><span class="p">())).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">_kl</span> <span class="o">+=</span> <span class="bp">self</span><span class="p">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">prior</span><span class="p">,</span> <span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span><span class="p">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_rho</span><span class="p">).</span><span class="n">cpu</span><span class="p">())).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">_kl</span>

    <span class="k">def</span> <span class="nf">kl_divergence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span> <span class="n">q</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
                      <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'reverse'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># either reverse or forward KL div.
</span>        <span class="k">if</span> <span class="n">kl_type</span> <span class="o">==</span> <span class="s">'reverse'</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_kl_divergence</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_kl_divergence</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">rsample</span><span class="p">(</span><span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># reparametrization trick
</span>        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">mu</span><span class="p">.</span><span class="n">size</span><span class="p">()).</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">mu</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">sigma</span>
</code></pre></div></div>

<p>In this post we will be using linear layers only, but the module can easily be extended to a convolutional layer by changing the layer function. The VI module can be given a prior as well as a posterior distribution. Dependend on whether a mixture of Gaussians shall be used as prior, the KL divergence is either computed analytically with the version provided by PyTorch or our MC KL divergence is used. Further we can specify, whether the forward or reverse KL divergence shall be computed. At last, it implements a function for performing reparametrization.</p>

<h4 id="local-reparametrization-trick-layer">(Local) Reparametrization Trick Layer</h4>

<p>Now we extend our VI module with the reparametrization trick and its local counterpart.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RTLayer</span><span class="p">(</span><span class="n">VIModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">layer_fct</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                 <span class="n">weight_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
                 <span class="n">bias_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'forward'</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">RTLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">layer_fct</span><span class="p">,</span>
                                      <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                      <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                      <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                      <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                      <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">)</span>
        <span class="c1"># these will be used for an easy extension to a convolutional layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># sample each weight with reparametrization trick
</span>        <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W_mu</span><span class="p">,</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W_rho</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rsample</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span><span class="p">,</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_rho</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="c1"># use this weight for forward pass
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_fct</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LRTLayer</span><span class="p">(</span><span class="n">VIModule</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">layer_fct</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                 <span class="n">weight_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
                 <span class="n">bias_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'forward'</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">LRTLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">layer_fct</span><span class="p">,</span>
                                       <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                       <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                       <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                       <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                       <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">)</span>
        <span class="c1"># these will be used for an easy extension to a convolutional layer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># first conduct forward pass for mean and variance
</span>        <span class="n">act_mu</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_fct</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_mu</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_sigma</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">W_rho</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_mu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">bias_var</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias_rho</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bias_var</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">act_std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1e-16</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer_fct</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias_var</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span><span class="p">))</span>
        <span class="c1"># sample from activation
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">rsample</span><span class="p">(</span><span class="n">act_mu</span><span class="p">,</span> <span class="n">act_std</span><span class="p">)</span>
</code></pre></div></div>

<p>The implementation for both the “normal” reparametrization layer (<code class="language-plaintext highlighter-rouge">RTLayer</code>) and the local reparametrization layer (<code class="language-plaintext highlighter-rouge">LRTLayer</code>) both look very similar. The only difference can be spotted in the forward function. As described earlier the RTLayer first samples the weight vector and then performs the forward pass. In contrast to that the LRTLayer first performs the forward pass for mean and variance and afterwards uses the reparametrization trick to sample from these activations.</p>

<h4 id="linear-layer">Linear Layer</h4>

<p>The extensions to real layers, linear and convolutional, is trivial now. The example for a linear layer can be found underneath, the implementation for the convolutional layers can be found at the end of the post.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LinearRT</span><span class="p">(</span><span class="n">RTLayer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'forward'</span><span class="p">):</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_featurs</span> <span class="o">=</span> <span class="n">out_features</span>

        <span class="n">weight_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">)</span>

        <span class="n">bias_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_features</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">LinearRT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="n">linear</span><span class="p">,</span>
                                        <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                        <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                        <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                        <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LinearLRT</span><span class="p">(</span><span class="n">LRTLayer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'forward'</span><span class="p">):</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_featurs</span> <span class="o">=</span> <span class="n">out_features</span>

        <span class="n">weight_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">)</span>

        <span class="n">bias_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_features</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">LinearLRT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="n">linear</span><span class="p">,</span>
                                        <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                        <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                        <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                        <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="loss-function">Loss Function</h4>

<p>Before we can start using these layers in a neural network, we must implement our loss function, the ELBO. As a loss function we use the full negative log likelihood. This has the advantage of being able to estimate the full uncertainty. We will not dive deeper into the derivations, this is done in another <a href="">post</a>. As a quick overview, uncertainty can be divided into uncertainty or noise inherent in the data, called aleatoric uncertainty, and uncertainty inherent in our model, termed epistemic uncertainty. The aleatoric uncertainty is captured implictly during training with our loss function and can also be estimated without employing Bayesian techniques. To capture epistemic uncertainty on the other hand we must impose distributions onto our parameters and follow the Bayesian approach.</p>

<p>To capture the aleatoric uncertainty our model \(\mathbf{f}_{\boldsymbol{\theta}}\) gains a new head</p>

\[\mathbf{f}_{\boldsymbol{\theta}}(\mathbf{x})=\left[ \hat{\mathbf{y}},\hat{\boldsymbol{\sigma}}^2 \right]\]

<p>and our loss function is then given by</p>

\[\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{D}\sum_i \frac{1}{2}\hat{\sigma}_i^2 \left( y_i - \hat{y}_i \right)^2 + \frac{1}{2}\log\hat{\sigma}_i^2 ~.\]

<p>Using this loss function we can implicitly learn the aleatoric uncertainty in our data (<a href="https://arxiv.org/abs/1703.04977">Kendall and Gal 2017</a>). For numerical stability in practice we let our model ouput the (negative) log variance as this allows for negative values as well:</p>

\[\mathcal{L}(\boldsymbol{\theta}) = \frac{1}{D}\sum_i\frac{1}{2}\exp \left( -\log \hat{\sigma}_i^2 \right) \left( y_i - \hat{y}_i \right)^2 - \frac{1}{2}\log\hat{\sigma}_i^2 ~.\]

<p>In Regression \(D\) is set to the number of samples in the mini-batch, while in classification \(D=1\). To also account for epistemic uncertainty we need to compute the variance of the output using Monte Carlo integration</p>

\[\mathrm{Var}\left[\mathbf{y}\right]=\underbrace{\frac{1}{T}\sum_{t=1}^{T}\hat{\mathbf{y}}_t^2-\left(\frac{1}{T}\sum_{t=1}^{T}\hat{\mathbf{y}}_t \right)^2}_{\textrm{epistemic}}+\underbrace{\boldsymbol{\sigma}}_{\textrm{aleatoric}} ~,\]

<p>where \(\boldsymbol{\sigma}\) denotes the aleatoric part estimated with the full negative log likelihood.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gaussian_nll</span><span class="p">(</span><span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">logvar</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'mean'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">logvar</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">mu</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">logvar</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">if</span> <span class="n">reduction</span><span class="o">==</span><span class="s">'mean'</span> <span class="k">else</span> <span class="n">loss</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">ELBO</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">train_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'regression'</span><span class="p">):</span>        
        <span class="nb">super</span><span class="p">(</span><span class="n">ELBO</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">train_size</span> <span class="o">=</span> <span class="n">train_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="s">'mean'</span> <span class="k">if</span> <span class="n">train_type</span> <span class="o">==</span> <span class="s">'regression'</span> <span class="k">else</span> <span class="s">'sum'</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">kl</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">gaussian_nll</span><span class="p">(</span><span class="n">inputs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">target</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">reduction</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">train_size</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>

<span class="k">def</span> <span class="nf">calc_uncert</span><span class="p">(</span><span class="n">preds</span><span class="p">:</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'mean'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">epi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">var</span><span class="p">(</span><span class="n">preds</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ale</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">preds</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">exp</span><span class="p">()</span>
    <span class="n">uncert</span> <span class="o">=</span> <span class="n">ale</span> <span class="o">+</span> <span class="n">epi</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s">'mean'</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ale</span><span class="p">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">epi</span><span class="p">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">uncert</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ale</span><span class="p">,</span> <span class="n">epi</span><span class="p">,</span> <span class="n">uncert</span>
</code></pre></div></div>

<h4 id="kl-divergence-reweighting">KL Divergence Reweighting</h4>

<p>The attentive reader may have noticed that we have reweighted both our likelihood as well as our KL divergence in the above code for the ELBO. A problem in training Bayesian neural networks with VI arises when we have a discrepancy between number of model parameters and data set size. Most often our number of parameters exceeds the number of training points leading to overfitting when performing maximum likelihood estimation. When utilizing the ELBO this means the magnitude of the KL divergence term, the regularizer, exceeds the likelihood cost and the training focusses on reducing the complexity instead of the likelihood. Thus, the KL divergence term must be reweighted by a factor \(\beta\):</p>

\[\textrm{ELBO}(q(\boldsymbol{\theta})) = \mathbb{E}_{\boldsymbol{\theta}\sim q} \log p(\mathcal{D}|\boldsymbol{\theta}) - \beta \mathrm{KL}(q(\boldsymbol{\theta})||p(\boldsymbol{\theta})) ~.\]

<p>A good value for \(\beta\) leads to an initial magnitude of the complexity cost comparable to the magnitude of the likelihood term. But considering only the likelihood disregards the number of model parameters, which highly influences the magnitude of the KL divergence. Following from this, the KL divergence must be scaled by the number of parameters to ensure its magnitude is only influenced by the approximation between posterior and prior. Reducing the number of parameters would otherwise decrease the complexity cost without increasing the model fit. At best, the likelihood is scaled by the number of data points in the data set to balance out both terms.</p>

<p>Since the use for the ELBO is motivated by enabling mini-batched training, the literature provides different \(\beta\) for training with mini-batches. The standard scaling factor was introduced by <a href="https://www.cs.toronto.edu/~graves/nips_2011.pdf">Graves (2011)</a>, which sets \(\beta=\frac{1}{M}\), where \(M\) denotes the number of mini-batches. A more sophisticated version is provided by <a href="https://arxiv.org/abs/1505.05424">Blundell et al. (2015)</a>:</p>

\[\beta_i = \frac{2^{M-i}}{2^M-1} ~, \quad i \in \{1,...,M\} ~,\]

<p>where \(i\) denotes the number of the current batch number. This condition ensures that \(\beta\) is not uniform across mini-batches but still sophisticates \(\sum_{i=1}^M \beta_i=1\). While assuming higher values for \(\beta\) at the beginning of training, the importance of the complexity costrapidly declines. By utilizing this approach the prior gets more influential in the beginning,before the training focuses on the likelihood, when more data is observed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Union</span>

<span class="k">def</span> <span class="nf">get_beta</span><span class="p">(</span><span class="n">beta_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
             <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">beta_type</span> <span class="o">==</span> <span class="s">"Blundell"</span><span class="p">:</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="n">m</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">beta_type</span> <span class="o">==</span> <span class="s">"Standard"</span><span class="p">:</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">m</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">beta_type</span>
    <span class="k">return</span> <span class="n">beta</span>
</code></pre></div></div>

<p>Before we conduct experiments with the above described VI framework, we will revisit another very popular technique for approximate variational inference in neural networks that gains its popularity from its simplicity and fewer computational requirements.</p>

<h2 id="monte-carlo-dropout">Monte Carlo Dropout</h2>

<p>Coming from the rather intuitive way of modelling each weight with a Gaussian distribution to incorporate uncertainty into the model parameters, Monte Carlo (MC) dropout is less instinctive but bears some strong advantages. <a href="https://arxiv.org/abs/1506.02142">Gal and Ghahramani (2016)</a> proposed dropout as approximate Bayesian inference. Their idea is basically simple, instead of just applying dropout during training to prevent overfitting dropout is also applied during testing. This makes the output of the model a random variable to and we are able to quantify uncertainty is described earlier.</p>

<p>The derivation relies on variational inference and is based on some heavy assumptions about the prior. For the full proof the interested reader is referred to <a href="http://proceedings.mlr.press/v48/gal16-supp.pdf">Gal and Ghahramani (2015)</a>.</p>

<p>An MC dropout layer can easily be implemented in PyTorch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MCDropout</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MCDropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="experiments">Experiments</h3>

<p>As we have now revisited the most popular approxmation techniques with variational inference, we will now take them to use for the problem of non-linear regression. So, we first must generate our training data:</p>

\[y = 10 \sin(2\pi x) + \epsilon \quad \textrm{with} \quad \epsilon \sim \mathcal{N}(0,2) ~.\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma</span>
    <span class="k">return</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="n">epsilon</span>

<span class="n">train_size</span> <span class="o">=</span> <span class="mi">92</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">train_size</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training data'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'True'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_22_0.png" alt="png" /></p>

<p>Next we will generate helper functions that simplify training lateron. Also, we defined our function that plots the uncertainty and the mean prediction of our models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.ndimage</span> <span class="kn">import</span> <span class="n">gaussian_filter1d</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">Adam</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">loss_fct</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">beta_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">optim</span><span class="p">:</span> <span class="n">Optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">,</span>
          <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="nb">list</span><span class="p">):</span>

    <span class="n">losses</span><span class="p">,</span> <span class="n">mses</span><span class="p">,</span> <span class="n">kls</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="n">optim</span> <span class="o">=</span> <span class="n">optim</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">total_loss</span><span class="p">,</span> <span class="n">total_kl</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">optim</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="n">out</span><span class="p">,</span> <span class="n">kl</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">[(</span><span class="n">batch_idx</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]])</span>

            <span class="n">beta</span> <span class="o">=</span> <span class="n">get_beta</span><span class="p">(</span><span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">batch_idx</span><span class="o">=</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">[(</span><span class="n">batch_idx</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]],</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optim</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">total_kl</span> <span class="o">+=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

            <span class="n">mse</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">[(</span><span class="n">batch_idx</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="p">:</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">]]).</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">mses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
        <span class="n">kls</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_kl</span><span class="p">)</span>
        <span class="n">grads</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">gradients</span><span class="p">())</span>

        <span class="n">pbar</span><span class="p">.</span><span class="n">set_description</span><span class="p">(</span><span class="s">'loss: %.6f'</span> <span class="o">%</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">mses</span><span class="p">,</span> <span class="n">kls</span><span class="p">,</span> <span class="n">grads</span>


<span class="k">def</span> <span class="nf">pred</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mc_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>

    <span class="n">y_preds</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">mc_samples</span><span class="p">)):</span>
            <span class="n">y_pred</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">y_preds</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_pred</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">y_preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[:,:,</span><span class="mi">0</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_preds</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">ale</span><span class="p">,</span> <span class="n">epi</span><span class="p">,</span> <span class="n">uncert</span> <span class="o">=</span> <span class="n">calc_uncert</span><span class="p">(</span><span class="n">y_preds</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ale</span><span class="p">,</span> <span class="n">epi</span><span class="p">,</span> <span class="n">uncert</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">]),</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">]),</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">ale</span><span class="p">,</span> <span class="n">epi</span><span class="p">,</span> <span class="n">uncert</span>


<span class="k">def</span> <span class="nf">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">x_train</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_train</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">ale</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">epi</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">uncert</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>

    <span class="n">ale</span><span class="p">,</span> <span class="n">epi</span><span class="p">,</span> <span class="n">uncert</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ale</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">epi</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">uncert</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'#D1895C'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Predictive mean'</span><span class="p">);</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training data'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">flatten</span><span class="p">(),</span>
                    <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">y_pred_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">ale</span> <span class="o">+</span> <span class="n">epi</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
                    <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">y_pred_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">ale</span> <span class="o">+</span> <span class="n">epi</span><span class="p">),</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
                    <span class="n">color</span><span class="o">=</span><span class="s">'#6C85B6'</span><span class="p">,</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Aleatoric uncertainty'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_test</span><span class="p">.</span><span class="n">flatten</span><span class="p">(),</span>
                    <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">y_pred_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">epi</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
                    <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">y_pred_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">epi</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
                    <span class="n">color</span><span class="o">=</span><span class="s">'#6C85B6'</span><span class="p">,</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Epistemic uncertainty'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">r'$x$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">r'$y$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>
</code></pre></div></div>

<h4 id="hyperparameters">Hyperparameters</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">LR</span> <span class="o">=</span> <span class="mf">0.08</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">200</span>
</code></pre></div></div>

<h3 id="frequentist-approach">Frequentist Approach</h3>

<p>Before we conduct experiments with variational inference, we will first take a look at the frequentist approach.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># as the training function expects the model to return the kl, we return 0 here
</span>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">()]).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_freq</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">loss_fct</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span> <span class="p">:</span> <span class="n">F</span><span class="p">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_freq</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                <span class="n">beta_type</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 3.760674: 100%|██████████| 200/200 [00:02&lt;00:00, 75.08it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">500</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">y_pred_mean_freq</span><span class="p">,</span> <span class="n">ale_freq</span><span class="p">,</span> <span class="n">epi_freq</span><span class="p">,</span> <span class="n">uncert_freq</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_freq</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_freq</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                  <span class="n">ale</span><span class="o">=</span><span class="n">ale_freq</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_freq</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_freq</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 4270.19it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_30_1.png" alt="png" /></p>

<p>As can be seen, the output of the frequentist model only has one defined value. Even in regions lacking training data the model confidently outputs one value as the only true one. Moving forward to a Bayesian approach, we will first conduct maximum a posterior inference utilizing the full negativ elog likelihood as loss function and by this equipping our model with the ability to quantify aleatoric uncertainty in the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_map</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">loss_fct</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span> <span class="p">:</span> <span class="n">gaussian_nll</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">out</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s">'mean'</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_map</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                   <span class="n">beta_type</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 2.771659: 100%|██████████| 200/200 [00:03&lt;00:00, 64.52it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_map</span><span class="p">,</span> <span class="n">ale_map</span><span class="p">,</span> <span class="n">epi_map</span><span class="p">,</span> <span class="n">uncert_map</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_map</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_map</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                  <span class="n">ale</span><span class="o">=</span><span class="n">ale_map</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_map</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_map</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 4455.30it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_33_1.png" alt="png" /></p>

<p>With this approach we are able to incorporate quantitative uncertainty estimates into the model’s prediction. The farther we get from the training data, the more the uncertainty in the model’s output increases. Still, this is only part of the uncertainty, we do still not consider the uncertainty inherent in the model. Thus, we will now move towards the fully Bayesian approach with variational inference.</p>

<h3 id="going-bayesian-with-monte-carlo-dropout">Going Bayesian with Monte Carlo Dropout</h3>

<p>The first experiments will be conducted with MC dropout to show its simplicity. This approach only needs a very simple adjustment, we only must use the MCDropout layer from above after every forward pass.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ModelDropout</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ModelDropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">mc_dropout</span> <span class="o">=</span> <span class="n">MCDropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mc_dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mc_dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mc_dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># as the training function expects the model to return the kl, we return 0 here
</span>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">()]).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>

<p>It is important to mention that we must always set a weight decay in the optimizer, since that determines the shape of our prior (together with the dropout rate).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">model_dropout</span> <span class="o">=</span> <span class="n">ModelDropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
<span class="n">loss_fct</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span> <span class="p">:</span> <span class="n">gaussian_nll</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">out</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="s">'mean'</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_dropout</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                   <span class="n">beta_type</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 3.081201: 100%|██████████| 200/200 [00:03&lt;00:00, 56.73it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_dropout</span><span class="p">,</span> <span class="n">ale_dropout</span><span class="p">,</span> <span class="n">epi_dropout</span><span class="p">,</span> <span class="n">uncert_dropout</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_dropout</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_dropout</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                  <span class="n">ale</span><span class="o">=</span><span class="n">ale_dropout</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_dropout</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_dropout</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="c1">#plt.savefig('dropout_uncert.pdf', bbox_inches='tight')
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 1353.97it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_38_1.png" alt="png" /></p>

<p>Dropout makes it possible to quantify both, aleatoric and epistemic, uncertainty.</p>

<h3 id="bayes-by-backprop">Bayes by Backprop</h3>

<p>We will now use the VI layer we have defined earlier. Since we are modeling out prior and posterior explicitly (at the cost of doubling the number of weights), we have more freedom in designing the training procedure. The only difference to the models above is that we now have a KL divergence that must be computed for each forward pass</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ModelVI</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">reparam</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'lrt'</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ModelVI</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">LinearVI</span> <span class="o">=</span> <span class="n">LinearLRT</span> <span class="k">if</span> <span class="n">reparam</span> <span class="o">==</span> <span class="s">'lrt'</span> <span class="k">else</span> <span class="n">LinearRT</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">LinearVI</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">LinearVI</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">LinearVI</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">kl_div</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">kl_div</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">kl</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">children</span><span class="p">():</span>
            <span class="n">kl</span> <span class="o">+=</span> <span class="n">m</span><span class="p">.</span><span class="n">kl</span>
        <span class="k">return</span> <span class="n">kl</span>

    <span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="p">()]).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>

<p>In our first training procedure we will use a \(\beta=10^{-2}\) for both the “standard” and local reparametrization trick.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prior</span> <span class="o">=</span> <span class="p">{</span><span class="s">'mu'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">}</span>

<span class="n">posteriors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'mu'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
    <span class="s">'rho'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">elbo</span> <span class="o">=</span> <span class="n">ELBO</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">loss_fct</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="n">elbo</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">kl</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

<span class="n">kls</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">mses</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>
</code></pre></div></div>

<h4 id="standard-reparametrization-trick">“Standard” Reparametrization Trick</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_vi_rt</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span> <span class="n">reparam</span><span class="o">=</span><span class="s">'rt'</span><span class="p">)</span>

<span class="n">no_params</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">model</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">p</span><span class="p">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">()]))</span>
<span class="n">beta_type</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">no_params</span><span class="p">(</span><span class="n">model_vi_rt</span><span class="p">)</span>

<span class="n">losses_rt</span><span class="p">,</span> <span class="n">mses_rt</span><span class="p">,</span> <span class="n">kls_rt</span><span class="p">,</span> <span class="n">grads_rt</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_rt</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                             <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 341.115906: 100%|██████████| 200/200 [00:08&lt;00:00, 22.25it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_rt</span><span class="p">,</span> <span class="n">ale_rt</span><span class="p">,</span> <span class="n">epi_rt</span><span class="p">,</span> <span class="n">uncert_rt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_rt</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_rt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                  <span class="n">ale</span><span class="o">=</span><span class="n">ale_rt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_rt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_rt</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 945.32it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_45_1.png" alt="png" /></p>

<h4 id="local-reparametrization-trick-1">Local Reparametrization Trick</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_vi_lrt</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>

<span class="n">beta_type</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">no_params</span><span class="p">(</span><span class="n">model_vi_lrt</span><span class="p">)</span>

<span class="n">losses_lrt</span><span class="p">,</span> <span class="n">mses_lrt</span><span class="p">,</span> <span class="n">kls_lrt</span><span class="p">,</span> <span class="n">grads_lrt</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                                 <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>

<span class="n">kls</span><span class="p">[</span><span class="s">r'\#weights'</span><span class="p">]</span> <span class="o">=</span> <span class="n">kls_lrt</span>
<span class="n">mses</span><span class="p">[</span><span class="s">r'\#weights'</span><span class="p">]</span> <span class="o">=</span> <span class="n">mses_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 286.557373: 100%|██████████| 200/200 [00:10&lt;00:00, 19.43it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert_lrt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                  <span class="n">ale</span><span class="o">=</span><span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_lrt</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="c1">#plt.savefig('ffg_uncert.pdf', bbox_inches='tight')
</span><span class="n">preds</span><span class="p">[</span><span class="s">r'\#weights'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_mean_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 623.07it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_48_1.png" alt="png" /></p>

<h4 id="comparing-gradients">Comparing Gradients</h4>

<p>With more training samples the figure underneath would get more meaningful, but also in this example it can bessen that the average gradients after each forward pass are lower for the LRT meaning our model does not oscillate that much around a lcoal optimum.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grads_rt</span><span class="p">)),</span> <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">grads_rt</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">r'RT'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grads_lrt</span><span class="p">)),</span> <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">grads_lrt</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">r'LRT'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">r'iteration'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">r'grad'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_50_0.png" alt="png" /></p>

<h3 id="scale-mixture-prior">Scale Mixture Prior</h3>

<p><a href="https://arxiv.org/abs/1505.05424">Blundell et al. (2015)</a> have shown that using a scale mixture prior (mixture of two Gaussians) can be benefitial for training, since then the model has the ability to have weights with a very small variance but can also match for uncertainty estimation the high variance part of the prior.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prior</span> <span class="o">=</span> <span class="p">{</span><span class="s">'mu'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="s">'sigma'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">100.</span><span class="p">],</span> <span class="s">'pi'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]}</span>

<span class="n">posteriors</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'mu'</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
    <span class="s">'rho'</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">model_vi_smp</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>

<span class="n">beta_type</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">no_params</span><span class="p">(</span><span class="n">model_vi_smp</span><span class="p">)</span>

<span class="n">losses_smp</span><span class="p">,</span> <span class="n">mses_smp</span><span class="p">,</span> <span class="n">kls_smp</span><span class="p">,</span> <span class="n">grads_smp</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_smp</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                       <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 192.050583: 100%|██████████| 200/200 [00:17&lt;00:00, 11.24it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_smp</span><span class="p">,</span> <span class="n">ale_smp</span><span class="p">,</span> <span class="n">epi_smp</span><span class="p">,</span> <span class="n">uncert_smp</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_smp</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_smp</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">ale</span><span class="o">=</span><span class="n">ale_smp</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_smp</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_smp</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 230.19it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_53_1.png" alt="png" /></p>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_53_2.png" alt="png" /></p>

<h3 id="different-betas">Different Betas</h3>

<p>Since the selection of \(\beta\) highly influences our training procedure, we will conduct experiments using different \(\beta\).</p>

<h4 id="graves">Graves</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prior</span> <span class="o">=</span> <span class="p">{</span><span class="s">'mu'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'sigma'</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">}</span>
<span class="n">beta_type</span> <span class="o">=</span> <span class="s">"Standard"</span>
<span class="n">model_vi_lrt_standard</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">mses_lrt_standard</span><span class="p">,</span> <span class="n">kls_lrt_standard</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_standard</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
<span class="n">kls</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">kls_lrt_standard</span>
<span class="n">mses</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">mses_lrt_standard</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 406.891388: 100%|██████████| 200/200 [00:10&lt;00:00, 18.78it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert_lrt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_standard</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">ale</span><span class="o">=</span><span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_lrt</span><span class="p">)</span>
<span class="n">preds</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_mean_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 555.56it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_56_1.png" alt="png" /></p>

<h4 id="blundell">Blundell</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">beta_type</span> <span class="o">=</span> <span class="s">"Blundell"</span>
<span class="n">model_vi_lrt_blundell</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">mses_lrt_blundell</span><span class="p">,</span> <span class="n">kls_lrt_blundell</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_blundell</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
<span class="n">kls</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">kls_lrt_blundell</span>
<span class="n">mses</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">mses_lrt_blundell</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 323.418182: 100%|██████████| 200/200 [00:10&lt;00:00, 18.80it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert_lrt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_blundell</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">ale</span><span class="o">=</span><span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_lrt</span><span class="p">)</span>
<span class="n">preds</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_mean_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 457.78it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_59_1.png" alt="png" /></p>

<h4 id="no-reweighting-beta1">No Reweighting (\(\beta=1\))</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">beta_type</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">model_vi_lrt_1</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">mses_lrt_1</span><span class="p">,</span> <span class="n">kls_lrt_1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_1</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                    <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
<span class="n">kls</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">kls_lrt_1</span>
<span class="n">mses</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">mses_lrt_1</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 527.517944: 100%|██████████| 200/200 [00:10&lt;00:00, 19.60it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert_lrt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_1</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">ale</span><span class="o">=</span><span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_lrt</span><span class="p">)</span>
<span class="n">preds</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_mean_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 653.01it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_62_1.png" alt="png" /></p>

<h4 id="no-kl-divergence-beta0">No KL divergence (\(\beta=0\))</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">beta_type</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">model_vi_lrt_0</span> <span class="o">=</span> <span class="n">ModelVI</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">mses_lrt_0</span><span class="p">,</span> <span class="n">kls_lrt_0</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_0</span><span class="p">,</span> <span class="n">loss_fct</span><span class="o">=</span><span class="n">loss_fct</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                    <span class="n">beta_type</span><span class="o">=</span><span class="n">beta_type</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">)</span>
<span class="n">kls</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">kls_lrt_0</span>
<span class="n">mses</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">mses_lrt_0</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>loss: 183.360672: 100%|██████████| 200/200 [00:10&lt;00:00, 19.51it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert_lrt</span> <span class="o">=</span> <span class="n">pred</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_vi_lrt_0</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">plot_uncert</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_pred_mean</span><span class="o">=</span><span class="n">y_pred_mean_lrt</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
            <span class="n">ale</span><span class="o">=</span><span class="n">ale_lrt</span><span class="p">,</span> <span class="n">epi</span><span class="o">=</span><span class="n">epi_lrt</span><span class="p">,</span> <span class="n">uncert</span><span class="o">=</span><span class="n">uncert_lrt</span><span class="p">)</span>
<span class="n">preds</span><span class="p">[</span><span class="n">beta_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_pred_mean_lrt</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:00&lt;00:00, 624.75it/s]
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_65_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s">'\#weights'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">'Standard'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'Blundell'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
<span class="k">for</span> <span class="n">beta</span><span class="p">,</span> <span class="n">kl</span> <span class="ow">in</span> <span class="n">kls</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kl</span><span class="p">)),</span> <span class="n">kl</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="n">colors</span><span class="p">[</span><span class="n">beta</span><span class="p">]])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1695</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">r'iteration'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">r'KL$q((\bm{\theta})||p(\bm{\theta}))$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'diff_betas_kl.pdf'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_66_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">beta</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">preds</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="n">colors</span><span class="p">[</span><span class="n">beta</span><span class="p">]])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'training data'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">r'x'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">r'y'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'diff_betas_pred.pdf'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_67_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">beta</span><span class="p">,</span> <span class="n">mse</span> <span class="ow">in</span> <span class="n">mses</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mse</span><span class="p">)),</span> <span class="n">gaussian_filter1d</span><span class="p">(</span><span class="n">mse</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="n">colors</span><span class="p">[</span><span class="n">beta</span><span class="p">]])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'upper right'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">r'iteration'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">r'MSE$(y;\hat{y})$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'diff_betas_nll.pdf'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s">'tight'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/imgs/BNNs_with_VI_files/BNNs_with_VI_68_0.png" alt="png" /></p>

<h2 id="appendix">Appendix</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">conv2d</span>
<span class="kn">from</span> <span class="nn">torch.nn.modules.utils</span> <span class="kn">import</span> <span class="n">_pair</span>

<span class="k">class</span> <span class="nc">Conv2dRT</span><span class="p">(</span><span class="n">RTLayer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                 <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'reverse'</span><span class="p">):</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="n">weight_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">bias_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_channels</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2dRT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">conv2d</span><span class="p">,</span>
                                        <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                        <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                        <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                        <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">,</span>
                                        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Conv2dLRT</span><span class="p">(</span><span class="n">LRTLayer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span>
                 <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                 <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                 <span class="n">prior</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">posteriors</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">kl_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">'reverse'</span><span class="p">):</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="n">weight_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">bias_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">out_channels</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2dLRT</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">layer_fct</span><span class="o">=</span><span class="n">conv2d</span><span class="p">,</span>
                                        <span class="n">weight_size</span><span class="o">=</span><span class="n">weight_size</span><span class="p">,</span>
                                        <span class="n">bias_size</span><span class="o">=</span><span class="n">bias_size</span><span class="p">,</span>
                                        <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
                                        <span class="n">posteriors</span><span class="o">=</span><span class="n">posteriors</span><span class="p">,</span>
                                        <span class="n">kl_type</span><span class="o">=</span><span class="n">kl_type</span><span class="p">,</span>
                                        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

  </div><a class="u-url" href="/linear/regression/vi/mcmc/2021/03/13/BNNs_with_VI.html" hidden></a>
</article>
<div class="footer">
 <div class="cookie-warning">
  <h1>Cookie Warning</h1>
  <p>I hereby have to draw your intention to the fact that this site uses cookies for a better interaction with this site. Nothing is saved for advertising.</p>
  <div class="cookie-btns">
   <input type="button" value="Got it" id="cookie-yes" />
   <input type="button" value="No thanks" id="cookie-no" />
  </div>
 </div>
 <div class="footer-personal">
  <h2>Malte Tölle</h2>
  <p>Machine Learning and Computer Vision</p>
 </div>
 <div class="footer-links">
 <a href="mailto:malte.toelle@gmail.com" ><img src="/assets/imgs/mail.svg" class="footer-social-media" /></a>
 <a href="/assets//pdfs/CV.pdf" ><img src="/assets/imgs/cv.svg" class="footer-social-media" /></a>
 <a href="https://github.com/maltetoelle" ><img src="/assets/imgs/github.svg" class="footer-social-media" /></a>
 <a href="https://www.linkedin.com/in/malte-t%C3%B6lle-96439b1a0/" ><img src="/assets/imgs/linkedin.svg" class="footer-social-media" /></a>
</div>

</div>


       </div>
      </div>

    </main><div class="footer">
 <div class="cookie-warning">
  <h1>Cookie Warning</h1>
  <p>I hereby have to draw your intention to the fact that this site uses cookies for a better interaction with this site. Nothing is saved for advertising.</p>
  <div class="cookie-btns">
   <input type="button" value="Got it" id="cookie-yes" />
   <input type="button" value="No thanks" id="cookie-no" />
  </div>
 </div>
 <div class="footer-personal">
  <h2>Malte Tölle</h2>
  <p>Machine Learning and Computer Vision</p>
 </div>
 <div class="footer-links">
 <a href="mailto:malte.toelle@gmail.com" ><img src="/assets/imgs/mail.svg" class="footer-social-media" /></a>
 <a href="/assets//pdfs/CV.pdf" ><img src="/assets/imgs/cv.svg" class="footer-social-media" /></a>
 <a href="https://github.com/maltetoelle" ><img src="/assets/imgs/github.svg" class="footer-social-media" /></a>
 <a href="https://www.linkedin.com/in/malte-t%C3%B6lle-96439b1a0/" ><img src="/assets/imgs/linkedin.svg" class="footer-social-media" /></a>
</div>

</div>
<script src="https://cdn.jsdelivr.net/npm/js-cookie@rc/dist/js.cookie.min.js"></script>
  <script src="/assets/js/script.js" ></script>
  </body>

</html>
